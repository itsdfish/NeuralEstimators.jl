# TODO Finish the documentation for all of the simulateX functions..


#TODO Why do I need to force m to be an integer?
"""
	simulate(parameters, m::Integer, J::Integer)

Simulates `J` sets of `m` independent replicates for each parameter vector in
`parameters` by calling `simulate(parameters, m)` a total of `J` times.
"""
function simulate(parameters, m::Integer, J::Integer)
	v = [simulate(parameters, m) for i ‚àà 1:J]
	v = vcat(v...) # should be ok since we're only splatting J vectors, which doesn't get prohibitively large even during bootstrapping. TODO No reason not to use stack, though.
	return v
end


# Wrapper function that returns simulated data and the true parameter values
_simulate(params::P, m) where {P <: Union{AbstractMatrix, ParameterConfigurations}} = (simulate(params, m), _extractŒ∏(params))



# ---- Gaussian process ----


"""
	simulategaussianprocess(L::AbstractArray{T, 2}, œÉ::T, m::Integer)
	simulategaussianprocess(L::AbstractArray{T, 2})

Simulates `m` realisations from a Gau(0, ùö∫ + œÉ¬≤ùêà) distribution, where ùö∫ ‚â° LL'.

If `œÉ` and `m` are not provided, a single field without nugget variance is returned.
"""
function simulategaussianprocess(L::AbstractArray{T, 2}, œÉ::T, m::Integer) where T
	n = size(L, 1)
	y = similar(L, n, m)
	for h ‚àà 1:m
		y[:, h] = simulategaussianprocess(L, œÉ)
	end
	return y
end

function simulategaussianprocess(L::AbstractArray{T, 2}, œÉ::T) where T
	n = size(L, 1)
	return simulategaussianprocess(L) + œÉ * randn(T, n)
end

function simulategaussianprocess(L::AbstractArray{T, 2}) where T
	n = size(L, 1)
	y = randn(T, n)
	return L * y
end


# ---- Schlather's max-stable model ----

"""
	simulateschlather(L::AbstractArray{T, 2}; C = 3.5)
	simulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)

Simulates from Schlather's max-stable model.
"""
function simulateschlather(L::AbstractArray{T, 2}; C = 3.5) where T <: Number

	n = size(L, 1)  # number of spatial locations

	Z   = fill(zero(T), n)
	Œ∂‚Åª¬π = randexp(T)
	Œ∂   = 1 / Œ∂‚Åª¬π

	# A property of the model that must be enforced is E(max{0, Y·µ¢}) = 1. It can
	# be shown that this condition is satisfied if the marginal variance of Y(‚ãÖ)
	# is equal to 2œÄ. Now, our simulation design embeds a marginal variance of 1
	# into fields generated from the cholesky factors, and hence
	# simulategaussianprocess(L) returns simulations from a Gaussian
	# process with marginal variance 1. To scale the marginal variance to
	# 2œÄ, we therefore need to multiply the field by ‚àö(2œÄ).

	# Note that, compared with Algorithm 1.2.2 of Dey DK, Yan J (2016),
	# some simplifications have been made to the code below. This is because
	# max{Z(s), Œ∂W(s)} ‚â° max{Z(s), max{0, Œ∂Y(s)}} = max{Z(s), Œ∂Y(s)}, since
	# Z(s) is initialised to 0 and increases during simulation.
	while (Œ∂ * C) > minimum(Z)
		Y = simulategaussianprocess(L)
		Y = ‚àö(2œÄ)Y
		Z = max.(Z, Œ∂ * Y)
		E = randexp(T)
		Œ∂‚Åª¬π += E
		Œ∂ = 1 / Œ∂‚Åª¬π
	end

	# Lenzi et al. used the log transform to stablise the variance, and this can
	# help avoid neural network collapse. Note that there is also a theoretical
	# justification for this transformation; it transforms from the data from
	# the unit Fr√©chet scale to the Gumbel scale, which is typically better behaved.
	Z = log.(Z) # TODO decide if this is what we want to do; can add an arguement transform::Bool = true.

	return Z
end

function simulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5) where T <: Number
	n = size(L, 1)
	Z = similar(L, n, m)
	for h ‚àà 1:m
		Z[:, h] = simulateschlather(L, C = C)
	end

	return Z
end


# ---- Conditional extremes ----

a(h, z; Œª, Œ∫) = z * exp(-(h / Œª)^Œ∫)
b(h, z; Œ≤, Œª, Œ∫) = 1 + a(h, z, Œª = Œª, Œ∫ = Œ∫)^Œ≤
delta(h; Œ¥‚ÇÅ) = 1 + exp(-(h / Œ¥‚ÇÅ)^2)

CÃÉ(h, œÅ, ŒΩ) = matern(h, œÅ, ŒΩ)
œÉÃÉ‚ÇÄ(h, œÅ, ŒΩ) = ‚àö(2 - 2 * CÃÉ(h, œÅ, ŒΩ))

Œ¶(q::T) where T <: Number = cdf(Normal(zero(T), one(T)), q)
t(yÃÉ‚ÇÄ‚ÇÅ, Œº, œÑ, Œ¥) = F‚Çõ‚Åª¬π(Œ¶(yÃÉ‚ÇÄ‚ÇÅ), Œº, œÑ, Œ¥)


"""
	simulateconditionalextremes(Œ∏::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s‚ÇÄ_idx::Integer, u::T) where T <: Number
	simulateconditionalextremes(Œ∏::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s‚ÇÄ_idx::Integer, u::T, m::Integer) where T <: Number

Simulates from the spatial conditional extremes model for parameters.

# Examples
```
S = rand(Float32, 10, 2)
D = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(S), s‚±º in eachrow(S)]
L = maternchols(D, 0.6f0, 0.5f0)
s‚ÇÄ = S[1, :]'
h = map(norm, eachslice(S .- s‚ÇÄ, dims = 1))
s‚ÇÄ_idx = findfirst(x -> x == 0.0, h)
u = 0.7f0
simulateconditionalextremes(Œ∏, L[:, :, 1], h, s‚ÇÄ_idx, u)
```
"""
function simulateconditionalextremes(
	Œ∏::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s‚ÇÄ_idx::Integer, u::T, m::Integer
	) where T <: Number

	n = size(L, 1)
	Z = similar(L, n, m)
	for k ‚àà 1:m
		Z[:, k] = simulateconditionalextremes(Œ∏, L, h, s‚ÇÄ_idx, u)
	end

	return Z
end


function simulateconditionalextremes(
	Œ∏::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s‚ÇÄ_idx::Integer, u::T
	) where T <: Number

	@assert length(Œ∏) == 8
	@assert s‚ÇÄ_idx > 0
	@assert s‚ÇÄ_idx <= length(h)
	@assert size(L, 1) == size(L, 2)
	@assert size(L, 1) == length(h)

	# Parameters associated with a(.) and b(.):
	Œ∫ = Œ∏[1]
	Œª = Œ∏[2]
	Œ≤ = Œ∏[3]
	# Covariance parameters associated with the Gaussian process
	œÅ = Œ∏[4]
	ŒΩ = Œ∏[5]
	# Location and scale parameters for the residual process
	Œº = Œ∏[6]
	œÑ = Œ∏[7]
	Œ¥‚ÇÅ = Œ∏[8]

	# Construct the parameter Œ¥ used in the Subbotin distribution:
	Œ¥ = delta.(h, Œ¥‚ÇÅ = Œ¥‚ÇÅ)

	# Observed datum at the conditioning site, Z‚ÇÄ:
	Z‚ÇÄ = u + randexp(T)

	# Simulate a mean-zero Gaussian random field with unit marginal variance,
    # independently of Z‚ÇÄ. Note that YÃÉ inherits the order of L. Therefore, we
	# can use s‚ÇÄ_idx to access s‚ÇÄ in all subsequent vectors.
	YÃÉ  = simulategaussianprocess(L)

	# Adjust the Gaussian process so that it is 0 at s‚ÇÄ
	YÃÉ‚ÇÄ = YÃÉ .- YÃÉ[s‚ÇÄ_idx]

	# Transform to unit variance:
	# œÉÃÉ‚ÇÄ = sqrt.(2 .- 2 *  matern.(h, œÅ, ŒΩ))
	YÃÉ‚ÇÄ‚ÇÅ = YÃÉ‚ÇÄ ./ œÉÃÉ‚ÇÄ.(h, œÅ, ŒΩ)
	YÃÉ‚ÇÄ‚ÇÅ[s‚ÇÄ_idx] = zero(T) # avoid pathology by setting YÃÉ‚ÇÄ‚ÇÅ(s‚ÇÄ) = 0.

	# Probability integral transform from the standard Gaussian scale to the
	# standard uniform scale, and then inverse probability integral transform
	# from the standard uniform scale to the Subbotin scale:
    Y = t.(YÃÉ‚ÇÄ‚ÇÅ, Œº, œÑ, Œ¥)

	# Apply the functions a(‚ãÖ) and b(‚ãÖ) to simulate data throughout the domain:
	Z = a.(h, Z‚ÇÄ, Œª = Œª, Œ∫ = Œ∫) + b.(h, Z‚ÇÄ, Œ≤ = Œ≤, Œª = Œª, Œ∫ = Œ∫) .* Y

	# Variance stabilising transform
	Z = cbrt.(Z) # TODO decide if this is what we want to do; can add an arguement transform::Bool = true.

	return Z
end




# ---- Miscellaneous functions ----

#TODO replace besselk with https://github.com/cgeoga/BesselK.jl
@doc raw"""
    matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)
For two points separated by `h` units, compute the Mat√©rn covariance function
with range `œÅ`, smoothness `ŒΩ`, and marginal variance `œÉ¬≤`.

We use the parametrisation
``C(\mathbf{h}) = \sigma^2 \frac{2^{1 - \nu}}{\Gamma(\nu)} \left(\frac{\|\mathbf{h}\|}{\rho}\right) K_\nu \left(\frac{\|\mathbf{h}\|}{\rho}\right)``,
where ``\Gamma(\cdot)`` is the gamma function, and ``K_\nu(\cdot)`` is the modified Bessel
function of the second kind of order ``\nu``. This parameterisation is the same as used by the `R`
package `fields`, but differs to the parametrisation given by Wikipedia.

Note that the `Julia` functions for ``\Gamma(\cdot)`` and ``K_\nu(\cdot)``, respectively `gamma()` and
`besselk()`, do not work on the GPU and, hence, nor does `matern()`.
"""
function matern(h, œÅ, ŒΩ, œÉ¬≤ = one(typeof(h)))

	@assert h >= 0 "h should be non-negative"
	@assert œÅ > 0 "œÅ should be positive"
	@assert ŒΩ > 0 "ŒΩ should be positive"

	if h == 0
        C = œÉ¬≤
    else
		d = h / œÅ
        C = œÉ¬≤ * ((2^(1 - ŒΩ)) / gamma(ŒΩ)) * d^ŒΩ * besselk(ŒΩ, d)
    end
    return C
end

matern(h, œÅ) =  matern(h, œÅ, 1.0)


# TODO a bit weird that we're forcing œÉ = 1
"""
    maternchols(D, œÅ, ŒΩ)
Given a distance matrix `D`, computes the covariance matrix under the
Mat√©rn covariance function with range `œÅ` and smoothness `ŒΩ`, and
returns the Cholesky factor of this covariance matrix.

Providing vectors for `œÅ` and `ŒΩ` will yield a three-dimensional array of
Cholesky factors.
"""
function maternchols(D, œÅ, ŒΩ)
	L = [cholesky(Symmetric(matern.(D, œÅ[i], ŒΩ[i]))).L  for i ‚àà eachindex(œÅ)]
	L = convert.(Array, L) # TODO Would be better if stackarrays() could handle other classes. Maybe it would work if I remove the type from stackarrays()
	L = stackarrays(L, merge = false)
	return L
end

"""
    _incgammalowerunregularised(a, x)
For positive `a` and `x`, computes the lower unregularised incomplete gamma
function, ``\\gamma(a, x) = \\int_{0}^x t^{a-1}e^{-t}dt``.
"""
_incgammalowerunregularised(a, x) = incgamma(a, x; upper = false, reg = false)
