<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced usage · NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../motivation/">Motivation</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../simpleexample/">Simple example</a></li><li><a class="tocitem" href="../morecomplicatedexample/">More complicated example</a></li><li class="is-active"><a class="tocitem" href>Advanced usage</a><ul class="internal"><li><a class="tocitem" href="#Loading-previously-saved-neural-estimators"><span>Loading previously saved neural estimators</span></a></li><li><a class="tocitem" href="#Computational-considerations"><span>Computational considerations</span></a></li><li><a class="tocitem" href="#Variable-sample-sizes"><span>Variable sample sizes</span></a></li><li><a class="tocitem" href="#Combining-neural-and-expert-summary-statistics"><span>Combining neural and expert summary statistics</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/core/">Core functions</a></li><li><a class="tocitem" href="../../API/simulation/">Simulation and density functions</a></li><li><a class="tocitem" href="../../API/utility/">Utility functions</a></li><li><a class="tocitem" href="../../API/">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Workflow</a></li><li class="is-active"><a href>Advanced usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced usage</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/workflow/advancedusage.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced-usage"><a class="docs-heading-anchor" href="#Advanced-usage">Advanced usage</a><a id="Advanced-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-usage" title="Permalink"></a></h1><h2 id="Loading-previously-saved-neural-estimators"><a class="docs-heading-anchor" href="#Loading-previously-saved-neural-estimators">Loading previously saved neural estimators</a><a id="Loading-previously-saved-neural-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-previously-saved-neural-estimators" title="Permalink"></a></h2><p>As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the <em>parameters</em> of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.</p><p><a href="../../API/core/#NeuralEstimators.train"><code>train</code></a> automatically saves the neural estimator&#39;s parameters; to load them, one may use the following code, or similar:</p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
ψ, ϕ = architecture(p)
θ̂ = DeepSet(ψ, ϕ)
Flux.loadparams!(θ̂, loadbestweights(path))</code></pre><p>Above, <code>architecture(p)</code> returns the architecture used during training, where <code>p</code> is the number of parameters in the statistical model; <code>Flux.loadparams!</code> loads the parameters of the best neural estimator saved in <code>path</code>, as determined <a href="../../API/utility/#NeuralEstimators.loadbestweights"><code>loadbestweights</code></a>.</p><h2 id="Computational-considerations"><a class="docs-heading-anchor" href="#Computational-considerations">Computational considerations</a><a id="Computational-considerations-1"></a><a class="docs-heading-anchor-permalink" href="#Computational-considerations" title="Permalink"></a></h2><h3 id="Balancing-time-and-memory-complexity-during-training"><a class="docs-heading-anchor" href="#Balancing-time-and-memory-complexity-during-training">Balancing time and memory complexity during training</a><a id="Balancing-time-and-memory-complexity-during-training-1"></a><a class="docs-heading-anchor-permalink" href="#Balancing-time-and-memory-complexity-during-training" title="Permalink"></a></h3><p>&quot;On-the-fly&quot; simulation refers to simulating new values for the parameters, θ, and/or the data, Z, continuously during training. &quot;Just-in-time&quot; simulation refers to simulating small batches of parameters and data, training the neural estimator with this small batch, and then removing the batch from memory.   </p><p>There are three variants of on-the-fly and just-in-time simulation, each with advantages and disadvantages.</p><ul><li>Resampling θ and Z every epoch. This approach is the most theoretically justified and has the best memory complexity, since both θ and Z can be simulated just-in-time, but it has the worst time complexity.</li><li>Resampling θ every x epochs, resampling Z every epoch. This approach can reduce time complexity if generating θ (or intermediate objects thereof) dominates the computational cost. Further, memory complexity may be kept low since Z can still be simulated just-in-time.</li><li>Resampling θ every x epochs, resampling Z every y epochs, where x is a multiple of y. This approach minimises time complexity but has the largest memory complexity, since both θ and Z must be stored in full. Note that fixing θ and Z (i.e., setting y = ∞) often leads to worse out-of-sample performance and, hence, is generally discouraged.</li></ul><p>The keyword arguments <code>epochs_per_θ_refresh</code> and <code>epochs_per_Z_refresh</code> in <code>train()</code> are intended to cater for these simulation variants.</p><h3 id="Sharing-intermediate-objects-between-parameter-configurations"><a class="docs-heading-anchor" href="#Sharing-intermediate-objects-between-parameter-configurations">Sharing intermediate objects between parameter configurations</a><a id="Sharing-intermediate-objects-between-parameter-configurations-1"></a><a class="docs-heading-anchor-permalink" href="#Sharing-intermediate-objects-between-parameter-configurations" title="Permalink"></a></h3><p>For some models, computationally expensive intermediate objects, such as Cholesky factors when working with Gaussian process models, can be shared between multiple parameter configurations (Gerber and Nychka, 2021), and this can significantly reduce the training time and alleviate memory pressure.</p><p>Recall that in the <a href="../morecomplicatedexample/#More-complicated-example">More complicated example</a> considered previously, we computed the Cholesky factor for each parameter configuration. However, for that model, the Cholesky factor depends only on <span>$\rho$</span> and, hence, we can modify our design to exploit this fact and significantly reduce the computational burden in generating <a href="../../API/core/#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a> objects. The following is one such approach.</p><p>The key to our approach is the inclusion of an additional field in <code>Parameters</code> that gives index of the Cholesky factor associated with each parameter configuration:</p><pre><code class="nohighlight hljs">struct Parameters &lt;: ParameterConfigurations
	θ
	chols
	chol_index
end</code></pre><p>Then, we adapt our <code>Parameters</code> constructor so that, instead of sampling <code>K</code> parameter pairs independently, we sample <code>K ÷ N</code> values of <code>ρ</code>, and then repeat these parameters so that the final parameter vector has length <code>K</code>. The advantage of this approach is clear, in that we need only compute <code>K ÷ N</code> Cholesky factors.</p><pre><code class="nohighlight hljs">function Parameters(ξ, K::Integer; N = 10)

	@assert K % N == 0

	Ω = ξ.Ω

	σ = rand(Ω.σ, N)
	ρ = rand(Ω.ρ, K ÷ N)

	chols = maternchols(ξ.D, ρ, 1)

	# Construct θ such that σ runs faster than ρ
	σ = repeat(σ, outer = K ÷ N)
	ρ = repeat(ρ, inner = N)
	θ = hcat(σ, ρ)&#39;

	Parameters(θ, chols, objectindices(chols, θ))
end</code></pre><p>The above constructor makes use of the the convenience function <a href="../../API/simulation/#NeuralEstimators.objectindices"><code>objectindices</code></a>, which computes the index of the Cholesky factor associated with each parameter configuration (and with intermediate objects more generally).</p><p>Note that the default subsetting method for <code>ParameterConfigurations</code> objects automatically handles cases like this; in some applications, however, it may be necessary to define an appropriate subsetting method by overloading <a href="../../API/core/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a>.</p><h2 id="Variable-sample-sizes"><a class="docs-heading-anchor" href="#Variable-sample-sizes">Variable sample sizes</a><a id="Variable-sample-sizes-1"></a><a class="docs-heading-anchor-permalink" href="#Variable-sample-sizes" title="Permalink"></a></h2><h3 id="Training-with-a-variable-sample-size"><a class="docs-heading-anchor" href="#Training-with-a-variable-sample-size">Training with a variable sample size</a><a id="Training-with-a-variable-sample-size-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-a-variable-sample-size" title="Permalink"></a></h3><p>We often wish to apply a neural estimator to range of sample sizes <code>m</code>, that is, we would like the estimator to be able to draw strength from a variable number of independent replicates. To this end, it is typically helpful to also train the neural estimator with variable <code>m</code>, and this does not materially alter the workflow, except that one also needs to define a method of <code>simulate</code> for variable <code>m</code>.</p><p>Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a uniform prior for <span>$M$</span>, the random variable corresponding to sample size.</p><pre><code class="nohighlight hljs">function simulate(parameters::Parameters, ξ, m::R) where {R &lt;: AbstractRange{I}} where I &lt;: Integer

	# Sample K sample sizes
	m̃ = rand(m, K)

	# Pseudocode demonstrating the basic workflow
	Z = [&lt;simulate m̃[k] fields&gt; for k ∈ 1:K]

	return Z
end</code></pre><p>Then, setting the argument <code>m</code> in <a href="../../API/core/#NeuralEstimators.train"><code>train</code></a> to be an integer range will train the neural estimator with the given variable sample sizes.</p><p>It&#39;s convenient to be able to switch back and forth between training with a fixed and variable sample size as we see fit, and this can be achieved by trivially defining a method for <code>m::Integer</code> (and note that such a method is needed for other parts of the workflow):</p><pre><code class="nohighlight hljs">simulate(parameters::Parameters, ξ, m::Integer) = simulate(parameters, ξ, range(m, m))</code></pre><h3 id="Piecewise-neural-estimators"><a class="docs-heading-anchor" href="#Piecewise-neural-estimators">Piecewise neural estimators</a><a id="Piecewise-neural-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Piecewise-neural-estimators" title="Permalink"></a></h3><p>See <a href="../../API/core/#NeuralEstimators.DeepSetPiecewise"><code>DeepSetPiecewise</code></a>.</p><p class="math-container">\[\hat{\mathbf{\theta}}(\mathcal{Z})
=
\begin{cases}
\hat{\mathbf{\theta}}_1(\mathcal{Z}) &amp; |\mathcal{Z}| \leq m_1,\\
\hat{\mathbf{\theta}}_2(\mathcal{Z}) &amp; m_1 &lt; |\mathcal{Z}| \leq m_2,\\
\quad \vdots \\
\hat{\mathbf{\theta}}_l(\mathcal{Z}) &amp; |\mathcal{Z}| &gt; m_{l-1}.
\end{cases}\]</p><h2 id="Combining-neural-and-expert-summary-statistics"><a class="docs-heading-anchor" href="#Combining-neural-and-expert-summary-statistics">Combining neural and expert summary statistics</a><a id="Combining-neural-and-expert-summary-statistics-1"></a><a class="docs-heading-anchor-permalink" href="#Combining-neural-and-expert-summary-statistics" title="Permalink"></a></h2><p>See <a href="../../API/core/#NeuralEstimators.DeepSetExpert"><code>DeepSetExpert</code></a>.</p><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><p>Bootstrapping is a powerful technique for estimating the distribution of an estimator and, hence, facilitating uncertainty quantification. Bootstrap methods are considered to be accurate but often too computationally expensive for traditional likelihood-based estimators, but are well suited to fast neural estimators. We implement bootstrapping with  <a href="../../API/core/#NeuralEstimators.parametricbootstrap"><code>parametricbootstrap</code></a> and <a href="../../API/core/#NeuralEstimators.nonparametricbootstrap"><code>nonparametricbootstrap</code></a>, with the latter also catering for so-called block bootstrapping.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../morecomplicatedexample/">« More complicated example</a><a class="docs-footer-nextpage" href="../../API/core/">Core functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Thursday 7 July 2022 15:45">Thursday 7 July 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
