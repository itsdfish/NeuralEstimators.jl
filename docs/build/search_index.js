var documenterSearchIndex = {"docs":
[{"location":"framework/#Framework","page":"Framework","title":"Framework","text":"","category":"section"},{"location":"framework/#Parameter-estimation","page":"Framework","title":"Parameter estimation","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"A statistical model is a set of probability distributions mathcalP on a sample space mathcalS. A parametric statistical model is one where the probability distributions in mathcalP are parameterised via some p-dimensional parameter vector mathbftheta, that is, where mathcalP equiv P_mathbftheta  mathbftheta in Theta, where Theta is the parameter space. Suppose that we have m mutually independent realisations from P_mathbftheta in mathcalP, which we collect in mathbfZ equiv (mathbfZ_1dotsmathbfZ_m). Then, the goal of parameter estimation is to infer the unknown mathbftheta from mathbfZ using an estimator,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"hatmathbftheta  mathcalS^m to Theta","category":"page"},{"location":"framework/#Bayes-estimators","page":"Framework","title":"Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"Estimators can be constructed intuitively within a decision-theoretic framework. Consider a non-negative loss function, L(mathbftheta hatmathbftheta(mathbfZ)), which quantifies the quality of an estimator hatmathbftheta(cdot) for a given mathbftheta and data set mathbfZ.    The estimator's risk function is the loss averaged over all possible data realisations. Assume, without loss of generality, that our sample space is mathcalS = mathbbR^n. Then, the risk function is","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" R(mathbftheta hatmathbftheta(cdot)) equiv int_mathcalS^m  L(mathbftheta hatmathbftheta(mathbfZ))p(mathbfZ mid mathbftheta) d mathbfZ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where p(mathbfZ mid mathbftheta) = prod_i=1^mp(mathbfZ_i mid mathbftheta) is the likelihood function. Now, a ubiquitous approach in estimator design is to minimise a weighted summary of the risk function known as the Bayes risk,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatmathbftheta(cdot))\n equiv int_Theta R(mathbftheta hatmathbftheta(cdot)) dOmega(mathbftheta)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where Omega(cdot) is a prior measure which, for ease of exposition, we will assume admits a density p(cdot) with respect to Lebesgue measure. The Bayes risk cannot typically be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of K parameters sampled from the prior Omega(cdot) denoted by vartheta  and, for each mathbftheta in vartheta, J sets of m mutually independent realisations from P_mathbftheta collected in mathcalZ_mathbftheta, then","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatmathbftheta(cdot))\n approx\nfrac1K sum_mathbftheta in vartheta bigg(frac1J sum_mathbfZ in mathcalZ_mathbftheta L(mathbftheta hatmathbftheta(mathbfZ))bigg)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"A minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdotcdot) and Omega(cdot).","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"Unique Bayes estimators are invariant to permutations of the conditionally independent data mathbfZ. Hence, we represent our neural estimators in the Deep Set framework, which is a universal representation for permutation-invariant functions. Specifically, we model our neural estimators as","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"hatmathbftheta(mathbfZ mathbfgamma) = mathbfphi(mathbfT(mathbfZ mathbfgamma) mathbfgamma) quad mathbfT(mathbfZ mathbfgamma)  \n= mathbfabig(mathbfpsi(mathbfZ_i mathbfgamma)  i = 1 dots mbig)","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where mathbfphi mathbbR^q to mathbbR^p and mathbfpsi mathbbR^n to mathbbR^q are neural networks whose parameters are collected in mathbfgamma, and mathbfa (mathbbR^q)^m to mathbbR^q is a permutation-invariant set function (typically elementwise addition, average, or maximum). Then, our neural estimator is hatmathbftheta(cdot mathbfgamma^*), where","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"mathbfgamma^*\nequiv\nundersetmathbfgammamathrmargmin  r_Omega(hatmathbftheta(cdot mathbfgamma))","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"with the Bayes risk approximated using Monte Carlo methods. Since the resulting neural estimator minimises (a Monte Carlo approximation of) the Bayes risk, we call it a neural Bayes estimator.","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"The neural Bayes estimator is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Define Omega(cdot), the prior distribution for mathbftheta.\nSample parameters from Omega(cdot) to form sets of parameters vartheta_rmtrain, vartheta_rmval, and vartheta_rmtest.\nSimulate data from the model, mathcalP, using these sets of parameters, yielding the data sets mathcalZ_rmtrain, mathcalZ_rmval, and mathcalZ_rmtest, respectively. \nChoose a loss function L(cdot cdot).\nDesign neural network architectures for mathbfphi(cdot mathbfgamma) and mathbfpsi(cdot mathbfgamma).\nUsing the training sets mathcalZ_textrmtrain and vartheta_rmtrain, train the neural network under L(cdotcdot) to obtain the neural Bayes estimator, hatmathbftheta(cdot mathbfgamma^*). During training, continuously monitor progress based on mathcalZ_textrmval and vartheta_rmval.\nAssess hatmathbftheta(cdot mathbfgamma^*) using mathcalZ_textrmtest and vartheta_rmtest.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"related/","page":"-","title":"-","text":"You may also be interested in the Julia packages Flux (the deep learning framework this package is built upon), Turing (for general-purpose probabilistic programming), and Mill (for generalised multiple-instance learning models). ","category":"page"},{"location":"API/utility/#Utility-functions","page":"Utility functions","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"loadbestweights\n\nstackarrays\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Utility functions","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nGiven a path to a training run containing neural networks saved with names 'networkepochx.bson' and an object saved as 'lossper_epoch.bson',  returns the weights of the best network (measured by validation loss).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Utility functions","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge::Bool = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary between arrays.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ∈ (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ∈ (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Utility functions","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Loading-previously-saved-neural-estimators","page":"Advanced usage","title":"Loading previously saved neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the parameters of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"train automatically saves the neural estimator's parameters; to load them, one may use the following code, or similar:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nusing Flux\nψ, ϕ = architecture(p)\nθ̂ = DeepSet(ψ, ϕ)\nFlux.loadparams!(θ̂, loadbestweights(path))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, architecture(p) returns the architecture used during training, where p is the number of parameters in the statistical model; Flux.loadparams! loads the parameters of the best neural estimator saved in path, as determined loadbestweights.","category":"page"},{"location":"workflow/advancedusage/#Computational-considerations","page":"Advanced usage","title":"Computational considerations","text":"","category":"section"},{"location":"workflow/advancedusage/#Balancing-time-and-memory-complexity-during-training","page":"Advanced usage","title":"Balancing time and memory complexity during training","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"\"On-the-fly\" simulation refers to simulating new values for the parameters, θ, and/or the data, Z, continuously during training. \"Just-in-time\" simulation refers to simulating small batches of parameters and data, training the neural estimator with this small batch, and then removing the batch from memory.   ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"There are three variants of on-the-fly and just-in-time simulation, each with advantages and disadvantages.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Resampling θ and Z every epoch. This approach is the most theoretically justified and has the best memory complexity, since both θ and Z can be simulated just-in-time, but it has the worst time complexity.\nResampling θ every x epochs, resampling Z every epoch. This approach can reduce time complexity if generating θ (or intermediate objects thereof) dominates the computational cost. Further, memory complexity may be kept low since Z can still be simulated just-in-time.\nResampling θ every x epochs, resampling Z every y epochs, where x is a multiple of y. This approach minimises time complexity but has the largest memory complexity, since both θ and Z must be stored in full. Note that fixing θ and Z (i.e., setting y = ∞) often leads to worse out-of-sample performance and, hence, is generally discouraged.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The keyword arguments epochs_per_θ_refresh and epochs_per_Z_refresh in train() are intended to cater for these simulation variants.","category":"page"},{"location":"workflow/advancedusage/#Sharing-intermediate-objects-between-parameter-configurations","page":"Advanced usage","title":"Sharing intermediate objects between parameter configurations","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For some models, computationally expensive intermediate objects, such as Cholesky factors when working with Gaussian process models, can be shared between multiple parameter configurations (Gerber and Nychka, 2021), and this can significantly reduce the training time and alleviate memory pressure.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Recall that in the TODO Gaussian Process model example, we computed the Cholesky factor for each parameter configuration. However, for that model, the Cholesky factor depends only on rho and, hence, we can modify our design to exploit this fact and significantly reduce the computational burden in generating ParameterConfigurations objects. The following is one such approach.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The key to our approach is the inclusion of an additional field in Parameters that gives the index of the Cholesky factor associated with each parameter configuration: Specifically, we add a pointer chol_idx where chol_idx[i] gives the Cholesky factor associated with parameter configuration θ[:, i].","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"struct Parameters{T, I} <: ParameterConfigurations\n\tθ::Matrix{T}\n\t\tchols::Array{Float64, 3}\n\tchol_idx::Vector{I}\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, we adapt our Parameters constructor so that each of the K parameter pairs are repeated J times. Since the parameters are repeated, we need only compute K Cholesky factors.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function Parameters(ξ, K::Integer; J::Integer = 10)\n\n\tΩ = ξ.Ω\n\n\tσ     = rand(Ω.σ, K)\n\tρ     = rand(Ω.ρ, K)\n\tchols = maternchols(ξ.D, ρ, 1)\n\n\t# Construct θ with σ and ρ repeated J times\n\tσ = repeat(σ, inner = J)\n\tρ = repeat(ρ, inner = J)\n\tθ = hcat(σ, ρ)'\n\n\t# Create a pointer for the Cholesky factors\n\tchol_idx = repeat(1:K, inner = J)\n\n\tParameters(θ, chols, \tchol_idx)\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that the default subsetting method for ParameterConfigurations objects automatically handles cases like this; in some applications, however, it may be necessary to define an appropriate subsetting method by overloading subsetparameters.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/#Training-with-a-variable-sample-size","page":"Advanced usage","title":"Training with a variable sample size","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"We often wish to apply a neural estimator to range of sample sizes m, that is, we would like the estimator to be able to draw strength from a variable number of independent replicates. To this end, it is typically helpful to also train the neural estimator with variable m, and this does not materially alter the workflow, except that one also needs to define a method of simulate for variable m.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a uniform prior for M, the random variable corresponding to sample size.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters::Parameters, ξ, m::R) where {R <: AbstractRange{I}} where I <: Integer\n\n\t# Sample K sample sizes\n\tm̃ = rand(m, K)\n\n\t# Pseudocode demonstrating the basic workflow\n\tZ = [<simulate m̃[k] fields> for k ∈ 1:K]\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, setting the argument m in train to be an integer range will train the neural estimator with the given variable sample sizes.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"It's convenient to be able to switch back and forth between training with a fixed and variable sample size as we see fit, and this can be achieved by trivially defining a method for m::Integer (and note that such a method is needed for other parts of the workflow):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"simulate(parameters::Parameters, ξ, m::Integer) = simulate(parameters, ξ, range(m, m))","category":"page"},{"location":"workflow/advancedusage/#Piecewise-neural-estimators","page":"Advanced usage","title":"Piecewise neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"See DeepSetPiecewise.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatmathbftheta(mathcalZ)\n=\nbegincases\nhatmathbftheta_1(mathcalZ)  mathcalZ leq m_1\nhatmathbftheta_2(mathcalZ)  m_1  mathcalZ leq m_2\nquad vdots \nhatmathbftheta_l(mathcalZ)  mathcalZ  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/#Combining-neural-and-expert-summary-statistics","page":"Advanced usage","title":"Combining neural and expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"See DeepSetExpert.","category":"page"},{"location":"workflow/advancedusage/#Bootstrapping","page":"Advanced usage","title":"Bootstrapping","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Bootstrapping is a powerful technique for estimating the distribution of an estimator and, hence, facilitating uncertainty quantification. Bootstrap methods are considered to be accurate but often too computationally expensive for traditional likelihood-based estimators, but are well suited to fast neural estimators. We implement bootstrapping with  parametricbootstrap and nonparametricbootstrap, with the latter also catering for so-called block bootstrapping.","category":"page"},{"location":"workflow/overview/#Workflow-overview","page":"Overview","title":"Workflow overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"To develop a neural estimator with NeuralEstimators.jl,","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Create an object ξ containing invariant model information, that is, model information that does not depend on the parameters and hence stays constant during training (e.g, the prior distribution of the parameters, spatial locations, distance matrices, etc.).\nDefine a subtype of ParameterConfigurations, say, Parameters (the name is arbitrary), containing a compulsory field θ storing K parameter vectors as a p × K matrix, with p the dimension of θ, as well as any other intermediate objects associated with the parameters (e.g., Cholesky factors) that are needed for data simulation.\nDefine a Parameters constructor Parameters(ξ, K::Integer), which draws K parameters from the prior.\nImplicitly define the statistical model by overloading the function simulate.\nInitialise neural networks ψ and ϕ, and a DeepSet object θ̂ = DeepSet(ψ, ϕ).\nTrain θ̂ using train under an arbitrary loss function.\nAssess θ̂ using assess.\nApply θ̂ to a real data set, using parametricbootstrap or nonparametricbootstrap to estimate the distribution of the estimator and, hence, facilitate uncertainty quantification.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"For clarity, see the Examples and, once familiar with the basic workflow, see Advanced usage for some practical considerations and how to construct neural estimators most effectively.","category":"page"},{"location":"API/simulation/#Data-simulation","page":"Simulation and density functions","title":"Data simulation","text":"","category":"section"},{"location":"API/simulation/","page":"Simulation and density functions","title":"Simulation and density functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by relying on the user to define the statistical model implicitly, either by providing data simulated from the model or by defining a function for data simulation. The following functions (in particular, their source code) serve as examples for how a user may formulate data simulation code for their own statistical model.","category":"page"},{"location":"API/simulation/#Model-simulators","page":"Simulation and density functions","title":"Model simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Simulation and density functions","title":"Simulation and density functions","text":"simulategaussianprocess\n\nsimulateschlather\n\nsimulateconditionalextremes","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Simulation and density functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::AbstractArray{T, 2}, σ::T, m::Integer)\nsimulategaussianprocess(L::AbstractArray{T, 2})\n\nSimulates m realisations from a Gau(0, 𝚺 + σ²𝐈) distribution, where 𝚺 ≡ LL'.\n\nIf σ and m are not provided, a single field without nugget variance is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Simulation and density functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::AbstractArray{T, 2}; C = 3.5)\nsimulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)\n\nSimulates from Schlather's max-stable model.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateconditionalextremes","page":"Simulation and density functions","title":"NeuralEstimators.simulateconditionalextremes","text":"simulateconditionalextremes(θ::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s₀_idx::Integer, u::T) where T <: Number\nsimulateconditionalextremes(θ::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, s₀_idx::Integer, u::T, m::Integer) where T <: Number\n\nSimulates from the spatial conditional extremes model for parameters.\n\nExamples\n\nS = rand(Float32, 10, 2)\nD = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S), sⱼ in eachrow(S)]\nL = maternchols(D, 0.6f0, 0.5f0)\ns₀ = S[1, :]'\nh = map(norm, eachslice(S .- s₀, dims = 1))\ns₀_idx = findfirst(x -> x == 0.0, h)\nu = 0.7f0\nsimulateconditionalextremes(θ, L[:, :, 1], h, s₀_idx, u)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Simulation and density functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Simulation and density functions","title":"Simulation and density functions","text":"gaussiandensity\n\nschlatherbivariatedensity\n\nSubbotin","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Simulation and density functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::A, L; logdensity::Bool = true) where {A <: AbstractArray{T, 1}} where T\ngaussiandensity(y::A, Σ; logdensity::Bool = true) where {A <: AbstractArray{T, N}} where {T, N}\n\nEfficiently computes the density function for y ~ 𝑁(0, Σ), with L the lower Cholesky factor of the covariance matrix Σ.\n\nThe second method assumes that the last dimension of y corresponds to the replicates dimension, and it exploits the fact that we need to compute the Cholesky factor L for these replicates once only.\n\nThe density function is\n\n2pimathbfSigma^-12 exp-frac12mathbfy^top mathbfSigma^-1mathbfy\n\nand the log-density is\n\n-fracn2ln2pi  -frac12lnmathbfSigma -frac12mathbfy^top mathbfSigma^-1mathbfy\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Simulation and density functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z₁, z₂, ψ; logdensity::Bool = true)\n\nThe bivariate density function for Schlather's max-stable model, as given in Raphaël Huser's PhD thesis (pg. 231-232) and Appendix C of the manuscript.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.Subbotin","page":"Simulation and density functions","title":"NeuralEstimators.Subbotin","text":"Subbotin(µ, τ, δ)\n\nThe Subbotin (delta-Laplace) distribution with location parameter μ, scale parameter τ>0, and shape parameter δ>0 has density, distribution, and quantile function,\n\n f_S(y mu tau delta) = fracdelta2tau Gamma(1delta) expleft(-leftfracy - mutauright^deltaright)\n F_S(y mu tau delta) = frac12 + textrmsign(y - mu) frac12 Gamma(1delta) gammaleft(1delta leftfracy - mutauright^deltaright)\n F_S^-1(p mu tau delta) = textsign(p - 05)G^-1left(2p - 05 frac1delta frac1(ktau)^deltaright)^1delta + mu\n\nwhere gamma(cdot) is the unnormalised incomplete lower gamma function and G^-1(cdot)  is the quantile function of the Gamma distribution.\n\nExamples\n\nd = Subbotin(0.7, 2, 2.5)\n\nlogpdf(d, 2.0)\ncdf(d, 2.0)\nquantile(d, 0.7)\n\n# Standard Gaussian distribution:\nμ = 0.0; τ = sqrt(2); δ = 2.0\nSubbotin(μ, τ, δ)\n\n# Standard Laplace distribution:\nμ = 0.0; τ = 1.0; δ = 1.0\nSubbotin(μ, τ, δ)\n\n\n\n\n\n","category":"type"},{"location":"API/simulation/#Miscellaneous-functions","page":"Simulation and density functions","title":"Miscellaneous functions","text":"","category":"section"},{"location":"API/simulation/","page":"Simulation and density functions","title":"Simulation and density functions","text":"matern\n\nmaternchols\n\nincgamma","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Simulation and density functions","title":"NeuralEstimators.matern","text":"matern(h, ρ, ν, σ² = 1)\n\nFor two points separated by h units, compute the Matérn covariance function with range ρ, smoothness ν, and marginal variance σ².\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright) K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu. This parameterisation is the same as used by the R package fields, but differs to the parametrisation given by Wikipedia.\n\nNote that the Julia functions for Gamma(cdot) and K_nu(cdot), respectively gamma() and besselk(), do not work on the GPU and, hence, nor does matern().\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Simulation and density functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, ρ, ν)\n\nGiven a distance matrix D, computes the covariance matrix Σ under the Matérn covariance function with range ρ and smoothness ν, and return the Cholesky factor of this matrix.\n\nProviding vectors for ρ and ν will yield a three-dimensional array of Cholesky factors.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.incgamma","page":"Simulation and density functions","title":"NeuralEstimators.incgamma","text":"incgamma(a::T, x::T; upper::Bool, reg::Bool) where {T <: AbstractFloat}\n\nFor positive parameter a and positive integration limit x, computes the incomplete gamma function, as described by the Wikipedia article.\n\nKeyword arguments:\n\nupper::Bool: if true, the upper incomplete gamma function is returned; otherwise, the lower version is returned.\nreg::Bool: if true, the regularized incomplete gamma function is returned; otherwise, the unregularized version is returned.\n\n\n\n\n\n","category":"function"},{"location":"#NeuralEstimators","page":"Home","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A neural estimator is a neural network that takes data as input, transforms them via a composition of nonlinear mappings, and provides a parameter estimate as an output. Once \"trained\", these likelihood-free estimators have two main advantages over conventional estimators: they are lightning fast with a predictable run-time and, since neural networks are universal function approximators, neural estimators can be expected to outperform constrained estimators (e.g., best linear unbiased estimators). Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is essentially available \"for free\" with a neural estimator, as the trained network can be reused repeatedly at almost no computational cost.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package NeuralEstimators aims to facilitate the development of neural estimators in a user-friendly manner. Rather than offering a selection of models for which neural estimators may be developed, NeuralEstimators facilitates neural estimation for arbitrary statistical models. This is achieved by having the user implicitly define their model by providing simulated data (or by defining a function for data simulation). Since only simulated data is needed, it is particularly straightforward to develop neural estimators for models with existing implementations, possibly in other programming languages (e.g., R or python).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install NeuralEstimators from Julia's package manager using the following command inside Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once familiar with the details of the Framework, see Workflow overview.","category":"page"},{"location":"#Supporting-and-citing","page":"Home","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use NeuralEstimators in your research or other activities, please use the following citation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{,\n  author = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Raphaël},\n  year = {2022},\n  title = {Fast Optimal Estimation with Intractable Models using Permutation-Invariant Neural Networks},\n  howpublished = {arXiv:2208.12942}\n}","category":"page"},{"location":"API/core/#Core-functions","page":"Core functions","title":"Core functions","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"This page documents the functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from storing parameters sampled from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.","category":"page"},{"location":"API/core/#Storing-parameters","page":"Core functions","title":"Storing parameters","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"ParameterConfigurations\n\nsubsetparameters","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core functions","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for storing parameters θ and any intermediate objects needed for data simulation with simulate.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.subsetparameters","page":"Core functions","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::Parameters, indices) where {Parameters <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nThe default method assumes that each field of parameters is an array. If the last dimension of the array has size equal to the number of parameter configurations, K, then the array is subsetted over its last dimension using indices; otherwise, the field is returned unchanged. If this default does not cover your use case, define an appropriate subsetting method by overloading subsetparameters after running import NeuralEstimators: subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Data-simulation","page":"Core functions","title":"Data simulation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"simulate","category":"page"},{"location":"API/core/#NeuralEstimators.simulate","page":"Core functions","title":"NeuralEstimators.simulate","text":"simulate(parameters::P, ξ, m::Integer, num_rep::Integer) where {P <: ParameterConfigurations}\n\nGeneric method that simulates num_rep sets of  sets of m independent replicates for each parameter configuration by calling simulate(parameters, ξ, m).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Neural-estimator-representations","page":"Core functions","title":"Neural estimator representations","text":"","category":"section"},{"location":"API/core/#Deep-Set-(vanilla)","page":"Core functions","title":"Deep Set (vanilla)","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSet\n\nDeepSet(ψ, ϕ; aggregation::String)","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSet","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ, agg)\n\nImplementation of the Deep Set framework, where ψ and ϕ are neural networks (e.g., Flux networks) and agg is a symmetric function that pools data over the last dimension (the replicates/batch dimension) of an array.\n\nDeepSet objects are applied to AbstractVectors of AbstractArrays, where each array is associated with one parameter vector.\n\nExamples\n\nn = 10 # observations in each realisation\np = 5  # number of parameters in the statistical model\nw = 32 # width of each layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ = Chain(Dense(w, w, relu), Dense(w, p));\nagg(X) = sum(X, dims = ndims(X))\nθ̂  = DeepSet(ψ, ϕ, agg)\n\n# A single set of m=3 realisations:\nZ = [rand(n, 1, 3)];\nθ̂ (Z)\n\n# Two sets each containing m=3 realisations:\nZ = [rand(n, 1, m) for m ∈ (3, 3)];\nθ̂ (Z)\n\n# Two sets respectivaly containing m=3 and m=4 realisations:\nZ = [rand(n, 1, m) for m ∈ (3, 4)];\nθ̂ (Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.DeepSet-Tuple{Any, Any}","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ; aggregation::String = \"mean\")\n\nConvenient constructor for a DeepSet object with agg equal to the \"mean\", \"sum\", or \"logsumexp\" function.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Deep-Set-(with-expert-summary-statistics)","page":"Core functions","title":"Deep Set (with expert summary statistics)","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSetExpert\n\nDeepSetExpert(deepset::DeepSet, ϕ, S)\n\nDeepSetExpert(ψ, ϕ, S; aggregation::String)\n\nsamplesize","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSetExpert","page":"Core functions","title":"NeuralEstimators.DeepSetExpert","text":"DeepSetExpert(ψ, ϕ, S, agg)\n\nImplementation of the Deep Set framework with ψ and ϕ neural networks, agg a symmetric function that pools data over the last dimension of an array, and S a vector of real-valued functions that serve as expert summary statistics.\n\nThe dimension of the domain of ϕ should be qₜ + qₛ, where qₜ is the range of ϕ and qₛ is the dimension of S, that is, length(S). DeepSetExpert objects are applied to AbstractVectors of AbstractArrays, where each array is associated with one parameter vector. The functions ψ and S both act on these arrays individually (i.e., they are broadcasted over the AbstractVector).\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.DeepSetExpert-Tuple{DeepSet, Any, Any}","page":"Core functions","title":"NeuralEstimators.DeepSetExpert","text":"DeepSetExpert(deepset::DeepSet, ϕ, S)\n\nDeepSetExpert constructor with the aggregation function agg and inner neural network ψ inherited from deepset.\n\nNote that we cannot inherit the outer network, ϕ, since DeepSetExpert objects require the dimension of the domain of ϕ to be qₜ + qₛ.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.DeepSetExpert-Tuple{Any, Any, Any}","page":"Core functions","title":"NeuralEstimators.DeepSetExpert","text":"DeepSetExpert(ψ, ϕ, S; aggregation::String = \"mean\")\n\nDeepSetExpert constructor with agg equal to the \"mean\", \"sum\", or \"logsumexp\" function.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.samplesize","page":"Core functions","title":"NeuralEstimators.samplesize","text":"samplesize(Z::A) where {A <: AbstractArray{T, N}} where {T, N}\n\nComputes the sample size m for a set of independent realisations Z, useful as an expert summary statistic in DeepSetExpert objects.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Piecewise-neural-estimators","page":"Core functions","title":"Piecewise neural estimators","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSetPiecewise","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSetPiecewise","page":"Core functions","title":"NeuralEstimators.DeepSetPiecewise","text":"DeepSetPiecewise(estimators, m_cutoffs)\n\nGiven an arbitrary number of estimators, creates a piecewise neural estimator based on the sample size cut offs, m_cutoffs, which should contain one element fewer than the number of estimators.\n\nExamples\n\nSuppose that we have two neural estimators, θ̂₁ and θ̂₂, taking the following arbitrary forms:\n\nn = 10\np = 5\nw = 32\n\nψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));\nθ̂₁ = DeepSet(ψ₁, ϕ₁)\n\nψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu));\nϕ₂ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, p));\nθ̂₂ = DeepSet(ψ₂, ϕ₂)\n\nFurther suppose that we've trained θ̂₁ for small sample sizes (e.g., m ≤ 30) and θ̂₂ for moderate-to-large sample sizes (e.g., m > 30). Then we construct a piecewise Deep Set object with a cut-off sample size of 30 which dispatches θ̂₁ if m ≤ 30 and θ̂₂ if m > 30:\n\nθ̂ = DeepSetPiecewise((θ̂₁, θ̂₂), (30,))\nZ = [rand(Float32, n, 1, m) for m ∈ (10, 50)]\nθ̂(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core functions","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"There are two training methods. For both methods, the validation parameters and validation data are held fixed so that the validation risk is interpretable. There are a number of practical considerations to keep in mind: In particular, see Balancing time and memory complexity during training.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"train","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core functions","title":"NeuralEstimators.train","text":"train(θ̂, ξ, P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator θ̂ by providing the invariant model information ξ needed for the constructor P to automatically sample the sets of training and validation parameters.\n\nKeyword arguments common to both train methods:\n\nm: sample sizes (either an Integer or a collection of Integers).\nbatchsize::Integer = 32\nepochs::Integer = 100: the maximum number of epochs used during training.\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nloss = mae: the loss function, which should return an average loss when applied to multiple replicates.\noptimiser = ADAM(1e-4)\nsavepath::String = \"runs/\": path to save the trained θ̂ and other information; if savepath is an empty string (i.e., \"\"), nothing is saved.\nsimulate_just_in_time::Bool = false: should we do \"just-in-time\" data simulation, which improves memory complexity at the cost of time complexity?\nstopping_epochs::Integer = 10: cease training if the risk doesn't improve in stopping_epochs epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nSimulator keyword arguments only:\n\nK::Integer = 10_000: the number of parameters in the training set; the size of the validation set is K ÷ 5.\nepochs_per_θ_refresh::Integer = 1: how often to refresh the training parameters; this must be a multiple of epochs_per_Z_refresh.\n\n\n\n\n\ntrain(θ̂, ξ, θ_train::P, θ_val::P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator θ̂ by providing the training and validation parameter sets explicitly as θ_train and θ_val, which are both held fixed during training, as well as the invariant model information ξ.\n\n\n\n\n\ntrain(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; <keyword args>) where {T, P <: ParameterConfigurations}\n\nTrain the neural estimator θ̂ by providing the training and validation parameter sets, θ_train and θ_val, and the training and validation data sets, Z_train and Z_val, all of which are held fixed during training.\n\nThe sample size argument m is inferred from Z_val. The training data Z_train can contain M replicates, where M is a multiple of m; the training data will then be recycled to imitate on-the-fly simulation. For example, if M = 50 and m = 10, epoch 1 uses the first 10 replicates, epoch 2 uses the second 10 replicates, and so on, until epoch 6 again uses the first 10 replicates.\n\nNote that the elements of Z_train and Z_val should be equally replicated; that is, the size of the last dimension in each array in Z_train should be constant, and similarly for Z_val.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessment","page":"Core functions","title":"Assessment","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"assess\n\nAssessment\n\nmerge(::Assessment)","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core functions","title":"NeuralEstimators.assess","text":"assess(estimators, ξ, parameters::P; <keyword args>) where {P <: ParameterConfigurations}\n\nUsing a collection of estimators, compute estimates from data simulated from a set of parameters with invariant information ξ.\n\nNote that assess() requires the user to have defined a method simulate(parameters, ξ, m::Integer).\n\nKeyword arguments\n\nm::Vector{Integer}: sample sizes to estimate from.\nestimator_names::Vector{String}: names of the estimators (sensible default values provided).\nparameter_names::Vector{String}: names of the parameters (sensible default values provided).\nJ::Integer = 1: the number of times to replicate each parameter in parameters.\nsave::Vector{String}: by default, no objects are saved; however, if save is provided, four DataFrames respectively containing the true parameters θ, estimates θ̂, runtimes, and merged θ and θ̂ will be saved in the directory save[1] with file names (not extensions) suffixed by save[2].\nuse_ξ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses the invariant model information, ξ: If it does, the estimator will be applied as estimator(Z, ξ).\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core functions","title":"NeuralEstimators.Assessment","text":"Assessment(θ, θ̂, runtime)\n\nA set of true parameters θ, corresponding estimates θ̂, and the runtime to obtain θ̂, as returned by a call to assess.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Base.merge-Tuple{Assessment}","page":"Core functions","title":"Base.merge","text":"merge(assessment::Assessment)\n\nMerge assessment into a single long-form DataFrame containing the true parameters and the corresponding estimates.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Bootstrapping","page":"Core functions","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Note that all bootstrapping functions are currently implemented for a single parameter configuration only.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"parametricbootstrap\n\nnonparametricbootstrap","category":"page"},{"location":"API/core/#NeuralEstimators.parametricbootstrap","page":"Core functions","title":"NeuralEstimators.parametricbootstrap","text":"parametricbootstrap(θ̂, parameters::P, ξ, m::Integer; B::Integer = 100, use_gpu::Bool = true) where {P <: ParameterConfigurations}\n\nReturns B parameteric bootstrap samples of an estimator θ̂ as a p × B matrix, where p is the number of parameters in the statistical model, based on data sets of size m simulated using the invariant model information ξ and parameter configurations, parameters.\n\nThis function requires the user to have defined a method simulate(parameters::P, ξ, m::Integer).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.nonparametricbootstrap","page":"Core functions","title":"NeuralEstimators.nonparametricbootstrap","text":"nonparametricbootstrap(θ̂, Z::AbstractArray{T, N}; B::Integer = 100, use_gpu::Bool = true)\nnonparametricbootstrap(θ̂, Z::AbstractArray{T, N}, blocks; B::Integer = 100, use_gpu::Bool = true)\n\nReturns B non-parametric bootstrap samples of an estimator θ̂ as a p × B matrix, where p is the number of parameters in the statistical model.\n\nThe argument blocks caters for block bootstrapping, and should be an integer vector specifying the block for each replicate. For example, if we have 5 replicates with the first two replicates corresponding to block 1 and the remaining replicates corresponding to block 2, then blocks should be [1, 1, 2, 2, 2]. The resampling algorithm tries to produce resampled data sets of a similar size to the original data, but this can only be achieved exactly if the blocks are the same length.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/#Univariate-Gaussian-data","page":"Examples","title":"Univariate Gaussian data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we consider a very simple estimation task, namely, inferring mu from N(mu sigma) data, where sigma is known. Specifically, we will develop a neural estimator for μ, where","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"mu sim N(0 05) quad mathcalZ equiv Z_1 dots Z_m  Z_i sim N(μ 1)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Before beginning, we load the required packages.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Distributions\nusing Flux","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Now we define the prior distribution, Omega(cdot), and sample parameters from it to form sets of parameters used for training, validating, and testing the estimator. In NeuralEstimators, parameters are stored as p times K matrices, where p is the number of parameters in the model and K is the number of sampled parameter vectors.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Ω = Normal(0, 0.5)\n\np = 1\nθ_train = rand(Ω, p, 10000)\nθ_val   = rand(Ω, p, 2000)  \nθ_test  = rand(Ω, p, 1000)  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model via simulated data. In the following, we overload the function simulate, but this is not necessary; one may simulate data however they see fit (e.g., using pre-existing functions, possibly from other programming languages).  Irrespective of its source, the data must be stored as a Vector of Arrays, with each array associated with one parameter vector. The dimension of these array must also be amenable to Flux neural networks (e.g., here we simulate 3-dimensional arrays, despite the second dimension being redundant), and one typically stores the data using Float32 precision for computational efficiency.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"import NeuralEstimators: simulate\n\n# m: number of independent replicates simulated for each parameter vector\nfunction simulate(θ_set, m::Integer)\n\tZ = [rand(Normal(θ[1], 1), 1, 1, m) for θ ∈ eachcol(θ_set)]\n\tZ = broadcast.(Float32, Z)\n\treturn Z\nend\n\nm = 15\nZ_train = simulate(θ_train, m)\nZ_val   = simulate(θ_val, m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We then design neural network architectures for use in the Deep Set framework, and we initialise the neural estimator as a DeepSet object.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"n = 1\nw = 32\nq = 16\nψ = Chain(Dense(n, w, relu), Dense(w, q, relu))\nϕ = Chain(Dense(q, w, relu), Dense(w, p), Flux.flatten)\nθ̂ = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss function.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ̂ = train(θ̂, θ_train, θ_val, Z_train, Z_val, epochs = 15)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The estimator θ̂ now approximates the Bayes estimator under the prior distribution Omega(cdot) and the absolute-error loss function and, hence, we refer to it as a neural Bayes estimator. To assess the performance of the estimator, one may use assess. Below, we assess the performance over a range of sample sizes.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z_test     = [simulate(θ_test, m) for m ∈ (5, 10, 15, 20, 30)]\nassessment = assess([θ̂], θ_test, Z_test)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The returned object is of type Assessment, and it contains the true parameters, estimates, and run times.  The true parameters and estimates may be merged into a convenient long-form DataFrame via merge, and this greatly facilitates visualisation and diagnostic computation. Further, NeuralEstimators provides several plotting methods.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"plotrisk(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, it is often helpful to visualise the empirical joint distribution of an estimator for a particular parameter configuration and a particular sample size.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"J = 100\nθ_scenario = rand(Ω, p, 1)\nZ_scenario = [simulate(θ_scenario, m, J)]\nassessment = assess([θ̂], θ_scenario, Z_scenario)  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The empirical joint distribution may then visualised as:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"plotdistribution(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The estimator may then be applied to real data, with bootstrapping facilitated with...","category":"page"}]
}
