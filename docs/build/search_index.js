var documenterSearchIndex = {"docs":
[{"location":"workflow/complex/#More-complicated-example","page":"More complicated example","title":"More complicated example","text":"","category":"section"},{"location":"workflow/complex/","page":"More complicated example","title":"More complicated example","text":"Show example requiring information to be passed around.","category":"page"},{"location":"workflow/simple/#Simple-example","page":"Simple example","title":"Simple example","text":"","category":"section"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Perhaps the simplest estimation task involves inferring μ from N(μ, σ) data, where σ = 1 is known.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The first step is to define an object that can be used to sample parameters and pass on information to the data simulation function. Here, we define the prior distribution, Ω, of θ, which we take to be a mean-zero Normal distribution standard deviation 0.5. In this simple example, only Ω is needed, but we wrap it in a Tuple for consistency with the general workflow.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Ω = Normal(0, 0.5)  \nξ = (Ω = Ω)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we define a sub-type of ParameterConfigurations with a field θ, which stores parameters as a p × K matrix, where p is the dimension of θ (here, p = 1).","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"struct Parameters{T} <: ParameterConfigurations\n\tθ::AbstractMatrix{T, 2}\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Storing parameter information in a struct is useful for storing intermediates objects needed for data simulation, such as Cholesky factors, and for implementing variants of on-the-fly and just-in-time simulation.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"We then define a Parameters constructor, which returns K draws from Ω.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"function Parameters(ξ, K::Integer)\n\tθ = rand(ξ.Ω, 1, K)\n\tParameters(θ)\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we implicitly define the statistical model by providing a method simulate(), which defines data simulation conditional on θ. The method must take two arguments; a Parameters object and m, the sample size. There is some flexibility in the permitted type of m (e.g., Integer, IntegerRange, etc.), but simulate() must return an AbstractVector of (multi-dimensional) AbstractArrays, where each array is associated with one parameter vector.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"import NeuralEstimators: simulate\nfunction simulate(params::Parameters, m::Integer)\n\tn = 1\n\tσ = 1\n\tθ = vec(params.θ)\n\tZ = [rand(Normal(μ, σ), n, 1, m) for μ ∈ θ]\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Note that the size of each array must be amenable to Flux neural networks; for instance, above we return a 3-dimensional array, even though the second dimension is redundant.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"We then choose an architecture for modelling ψ(⋅) and ϕ(⋅) in the Deep Set framework, and initialise the neural estimator as a DeepSet object.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"p = 1\nw = 32\nq = 16\nψ = Chain(Dense(n, w, relu), Dense(w, q, relu))\nϕ = Chain(Dense(q, w, relu), Dense(w, p), flatten)\nθ̂ = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we train the neural estimator using train(). This optimisation is performed with respect to an arbitrary loss function (default absolute error loss). The argument m specifies the sample size used during training; the type of m should be consistent with the simulate() method defined above.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"θ̂ = train(θ̂, Ω, m = 10)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The estimator θ̂ now approximates the Bayes estimator for θ. It's usually a good idea to assess the performance of the estimator before putting it into practice. Since the performance of θ̂ for particular values of θ may be of particular interest, estimate() takes an instance of Parameters.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"parameters = Parameters(Ω, 500)      # test set with 500 parameters\nm  = [1, 10, 30]                     # sample sizes we wish to test\ndf = estimate(θ̂, parameters, m = m)  ","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The true parameters, estimates, and timings from this test run are returned in a convenient DataFrame, ready for visualisation.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"API/utility/#Utility-functions","page":"Utility functions","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"loadbestweights\n\nstack\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Utility functions","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nGiven a path to a training run containing neural networks saved with names 'networkepochx.bson' and an object saved as 'lossper_epoch.bson',  returns the weights of the best network (measured by validation loss).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stack","page":"Utility functions","title":"NeuralEstimators.stack","text":"stack(v::V; merge::Bool = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary between arrays.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ∈ (1, 1)];\nstack(Z)\nstack(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ∈ (1, 2)];\nstack(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Utility functions","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"workflow/advanced/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advanced/#Reusing-intermediate-objects-(e.g.,-Cholesky-factors)-for-multiple-parameter-configurations","page":"Advanced usage","title":"Reusing intermediate objects (e.g., Cholesky factors) for multiple parameter configurations","text":"","category":"section"},{"location":"workflow/advanced/#Balancing-time-and-memory-complexity","page":"Advanced usage","title":"Balancing time and memory complexity","text":"","category":"section"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"\"On-the-fly\" simulation refers to simulating new values for the parameters, θ, and/or the data, Z, continuously during training. \"Just-in-time\" simulation refers to simulating small batches of parameters and data, training the neural estimator with this small batch, and then removing the batch from memory.   ","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"There are three variants of on-the-fly and just-in-time simulation, each with advantages and disadvantages.","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"Resampling θ and Z every epoch. This approach is the most theoretically justified and has the best memory complexity, since both θ and Z can be simulated just-in-time, but it has the worst time complexity.\nResampling θ every x epochs, resampling Z every epoch. This approach can reduce time complexity if generating θ (or intermediate objects thereof) dominates the computational cost. Further, memory complexity may be kept low since Z can still be simulated just-in-time.\nResampling θ every x epochs, resampling Z every y epochs, where x is a multiple of y. This approach minimises time complexity but has the largest memory complexity, since both θ and Z must be stored in full. Note that fixing θ and Z (i.e., setting y = ∞) often leads to worse out-of-sample performance and, hence, is generally discouraged.","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"The keyword arguments epochs_per_θ_refresh and epochs_per_Z_refresh in train() are intended to cater for these simulation variants.","category":"page"},{"location":"workflow/advanced/#Loading-previously-saved-estimators","page":"Advanced usage","title":"Loading previously saved estimators","text":"","category":"section"},{"location":"workflow/overview/#Workflow-overview","page":"Overview","title":"Workflow overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"A general overview for developing a neural estimator with NeuralEstimators.jl is as follows.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Create an object ξ that contains the information needed to sample the p-dimensional parameter vector θ.\nDefine a type Parameters <: ParameterConfigurations used to store information needed for data simulation. Parameters must contain a field θ, which stores K parameter vectors as a p × K matrix.\nDefine a Parameters constructor, Parameters(ξ, K::Integer).  \nImplicitly define the statistical model by providing a method simulate(parameters::Parameters, m) which simulates m independent realisations from the statistical model.\nInitialise neural networks ψ and ϕ. These will typically be Flux.jl networks.\nInitialise a DeepSet object, θ̂ = DeepSet(ψ, ϕ).\nTrain θ̂ using train() under an arbitrary loss function.\nTest θ̂ using estimate().\nApply θ̂ to real-world data.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"For clarity, see a Simple example and a More complicated example.","category":"page"},{"location":"API/simulation/#Data-simulation","page":"Data simulation","title":"Data simulation","text":"","category":"section"},{"location":"API/simulation/#Model-simulators","page":"Data simulation","title":"Model simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Data simulation","title":"Data simulation","text":"simulategaussianprocess\n\nsimulateschlather\n\nsimulateconditionalextremes","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Data simulation","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::AbstractArray{T, 2}, σ²::T, m::Integer)\n\nSimulates m realisations from a Gau(0, 𝚺 + σ²𝐈) distribution, where 𝚺 ≡ LL'.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Data simulation","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::AbstractArray{T, 2}; C = 3.5)\nsimulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)\n\nSimulates from Schlather's max-stable model. Based on Algorithm 1.2.2 of Dey DK, Yan J (2016). Extreme value modeling and risk analysis: methods and applications. CRC Press, Boca Raton, Florida.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateconditionalextremes","page":"Data simulation","title":"NeuralEstimators.simulateconditionalextremes","text":"simulateconditionalextremes(θ, L::AbstractArray{T, 2}, h, s₀, u;)\nsimulateconditionalextremes(θ, L::AbstractArray{T, 2}, h, s₀, m::Integer)\n\nSimulates from the spatial conditional extremes model.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Intermediate-objects","page":"Data simulation","title":"Intermediate objects","text":"","category":"section"},{"location":"API/simulation/","page":"Data simulation","title":"Data simulation","text":"matern\n\nmaternchols\n\nincgammalower\n\nfₛ","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Data simulation","title":"NeuralEstimators.matern","text":"matern(h, ρ, ν, σ² = 1)\n\nFor two points separated by h units, compute the Matérn covariance function with range ρ, smoothness ν, and marginal variance σ².\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright) K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu. This parameterisation is the same as used by the R package fields, but differs to the parametrisation given by Wikipedia.\n\nNote that the Julia functions for Gamma(cdot) and K_nu(cdot), respectively gamma() and besselk(), do not work on the GPU and, hence, nor does matern().\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Data simulation","title":"NeuralEstimators.maternchols","text":"maternchols(D, ρ, ν)\n\nGiven a distance matrix D, compute corresponding covariance matrix Σ under the Matérn covariance function with range ρ and smoothness ν, and return the Cholesky factor of this matrix.\n\nProviding vectors for ρ and ν will yield a three-dimensional array of Cholesky factors.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.incgammalower","page":"Data simulation","title":"NeuralEstimators.incgammalower","text":"incgammalower(a, x)\n\nFor positive a and x, computes the lower incomplete gamma function, gamma(a x) = int_0^x t^a-1e^-tdt.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.fₛ","page":"Data simulation","title":"NeuralEstimators.fₛ","text":"fₛ(x, μ, τ, δ)\nFₛ(q, μ, τ, δ)\nFₛ⁻¹(p, μ, τ, δ)\n\nThe density, distribution, and quantile functions Subbotin (delta-Laplace) distribution with location parameter μ, scale parameter τ, and shape parameter δ:\n\n f_S(y mu tau delta) = fracdelta2tau Gamma(1delta) expleft(-leftfracy - mutauright^deltaright)\n F_S(y mu tau delta) = frac12 + textrmsign(y - mu) frac12 Gamma(1delta) gammaleft(1delta leftfracy - mutauright^deltaright)\n F_S^-1(p mu tau delta) = textsign(p - 05)G^-1left(2p - 05 frac1delta frac1(ktau)^deltaright)^1delta + mu\n\nwith gamma(cdot) and G^-1(cdot) the unnormalised incomplete lower gamma function and quantile function of the Gamma distribution, respectively.\n\nExamples\n\np = [0.025, 0.05, 0.5, 0.9, 0.95, 0.975]\n\n# Standard Gaussian:\nμ = 0.0; τ = sqrt(2); δ = 2.0\nFₛ⁻¹.(p, μ, τ, δ)\n\n# Standard Laplace:\nμ = 0.0; τ = 1.0; δ = 1.0\nFₛ⁻¹.(p, μ, τ, δ)\n\n\n\n\n\n","category":"function"},{"location":"#NeuralEstimators-documentation","page":"Home","title":"NeuralEstimators documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Landing page and brief description of neural estimators as a recent likelihood-inference approach, and an alternative to ABC.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For those new to Julia, you may also be interested in the Julia packages Flux (the deep learning framework this package is built upon), Turing (general-purpose probabilistic programming), and Mill (caters for general multi-instance learning models, i.e., Deep Sets). ","category":"page"},{"location":"motivation/#Motivation","page":"Motivation","title":"Motivation","text":"","category":"section"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Definition of an estimator:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"hatmathbftheta  mathcalS^m to Theta","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Permutation invariance:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"hatmathbftheta(mathbfZ_1 dots mathbfZ_m) = hatmathbftheta(mathbfZ_pi(1) dots mathbfZ_pi(m))","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Under some arbitrary loss function L(mathbftheta hatmathbftheta(mathcalZ)), the risk function:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"R(mathbftheta hatmathbftheta(cdot)) equiv int_mathcalS^m  L(mathbftheta hatmathbftheta(mathcalZ))p(mathcalZ mid mathbftheta) d mathcalZ","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Weighted average risk function:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"r_Omega(hatmathbftheta(cdot))\nequiv int_Theta R(mathbftheta hatmathbftheta(cdot)) dOmega(mathbftheta)  ","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Deep Set (Zaheer et al., 2017) representation of an estimator:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"beginaligned\nhatmathbftheta(mathcalZ) = mathbfphi(mathbfT(mathcalZ)) \nmathbfT(mathcalZ)  = sum_mathbfZ in mathcalZ mathbfpsi(mathbfZ)\nendaligned","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Optimisation task:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"hatmathbftheta_mathbfgamma^*(cdot)\nmathbfgamma^*\nequiv\nundersetmathbfgammamathrmargmin  r_Omega(hatmathbftheta_mathbfgamma(cdot))","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"Monte Carlo approximation of the weighted average risk:","category":"page"},{"location":"motivation/","page":"Motivation","title":"Motivation","text":"r_Omega(hatmathbftheta(cdot))\napprox\nfrac1K sum_k = 1^K frac1J sum_j = 1^J L(mathbftheta_k hatmathbftheta(mathcalZ_kj))  ","category":"page"},{"location":"API/core/#Core-functions","page":"Core functions","title":"Core functions","text":"","category":"section"},{"location":"API/core/#Parameters","page":"Core functions","title":"Parameters","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"ParameterConfigurations\n\nsubsetparameters","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core functions","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for storing parameters θ and any intermediate objects needed for data simulation with simulate.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.subsetparameters","page":"Core functions","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::Parameters, indices) where {Parameters <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nThe default method assumes that each field of parameters is an array with the last dimension corresponding to the parameter configurations (i.e., it subsets over the last dimension of each array). If this is not the case, define an appropriate subsetting method by overloading subsetparameters after running import NeuralEstimators: subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Simulation","page":"Core functions","title":"Simulation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"simulate","category":"page"},{"location":"API/core/#NeuralEstimators.simulate","page":"Core functions","title":"NeuralEstimators.simulate","text":"simulate(parameters::P, m::Integer, num_rep::Integer) where {P <: ParameterConfigurations}\n\nGeneric method that simulates num_rep sets of  sets of m independent replicates for each parameter configuration by calling simulate(parameters, m).\n\nSee also Data simulation.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Deep-Set-representation","page":"Core functions","title":"Deep Set representation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSet\n\nDeepSet(ψ, ϕ; aggregation::String)","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSet","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ, agg)\n\nImplementation of the Deep Set framework, where ψ and ϕ are neural networks (e.g., Flux networks) and agg is a symmetric function that pools data over the last dimension (the replicates/batch dimension) of an array.\n\nDeepSet objects are applied to AbstractVectors of AbstractArrays, where each array is associated with one parameter vector.\n\nExamples\n\nn = 10 # observations in each realisation\np = 5  # number of parameters in the statistical model \nw = 32 # width of each layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ = Chain(Dense(w, w, relu), Dense(w, p));\nagg(X) = sum(X, dims = ndims(X))\nθ̂  = DeepSet(ψ, ϕ, agg)\n\n# A single set of m=3 realisations:\nZ = [rand(n, 1, 3)];\nθ̂ (Z)\n\n# Two sets each containing m=3 realisations:\nZ = [rand(n, 1, m) for m ∈ (3, 3)];\nθ̂ (Z)\n\n# Two sets respectivaly containing m=3 and m=4 realisations:\nZ = [rand(n, 1, m) for m ∈ (3, 4)];\nθ̂ (Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.DeepSet-Tuple{Any, Any}","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ; aggregation::String = \"mean\")\n\nConvenient constructor for a DeepSet object with agg equal to the \"mean\", \"sum\", or \"log-sum-exp\" function.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Training","page":"Core functions","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"There are two training methods. For both methods, the validation parameters and validation data are held fixed so that the validation risk is interpretable. There are a number of practical considerations to keep in mind: In particular, see Balancing time and memory complexity.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"train","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core functions","title":"NeuralEstimators.train","text":"train(θ̂, ξ, P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator θ̂ by providing the objects ξ needed for the constructor P to automatically sample the set of training and validation parameters, which may be refreshed periodically throughout training via the keyword argument epochs_per_θ_refresh.\n\nKeyword arguments common to both train methods:\n\nm: sample sizes (either an Integer or a collection of Integers).\nbatchsize::Integer = 32\nepochs::Integer = 100: the maximum number of epochs used during training.\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nloss = mae: the loss function, which should return an average loss when applied to multiple replicates.\noptimiser = ADAM(1e-4)\nsavepath::String = \"runs/\": path to save the trained θ̂ and other information.\nstopping_epochs::Integer = 10: halt training if the risk doesn't improve in stopping_epochs epochs.\nuse_gpu::Bool = true\n\nSimulator keyword arguments only:\n\nconfigs_per_epoch::Integer = 10_000: how many parameters constitute a single epoch.\nepochs_per_θ_refresh::Integer = 1: how often to refresh the training parameters; this must be a multiple of epochs_per_Z_refresh.\n\n\n\n\n\ntrain(θ̂, θ_train::P, θ_val::P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator θ̂ by providing the training and validation sets explicitly as θ_train and θ_val, which are both held fixed during training.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Estimation","page":"Core functions","title":"Estimation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"estimate","category":"page"},{"location":"API/core/#NeuralEstimators.estimate","page":"Core functions","title":"NeuralEstimators.estimate","text":"estimate(estimators, parameters::P, m; <keyword args>) where {P <: ParameterConfigurations}\n\nUsing a collection of estimators, compute estimates from data simulated from a set of parameters.\n\nestimate() requires the user to have defined a method simulate(parameters, m::Integer).\n\nKeyword arguments\n\nm::Vector{Integer} where I <: Integer: sample sizes to estimate from.\nestimator_names::Vector{String}: estimator names used when combining estimates into a DataFrame (e.g., [\"NeuralEstimator\", \"BayesEstimator\", \"MLE\"]), with sensible default values provided.\nparameter_names::Vector{String}: parameter names used when combining estimates into a DataFrame (e.g., [\"μ\", \"σ\"]), with sensible default values provided.\nnum_rep::Integer = 1: the number of times to replicate each parameter in parameters to reduce the effect of sample variability when assessing the estimators.\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Bootstrapping","page":"Core functions","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Note that all bootstrapping functions are currently implemented for a single parameter configuration only.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"parametricbootstrap\n\nnonparametricbootstrap","category":"page"},{"location":"API/core/#NeuralEstimators.parametricbootstrap","page":"Core functions","title":"NeuralEstimators.parametricbootstrap","text":"parametricbootstrap(θ̂, parameters::P, m::Integer; B::Integer = 100, use_gpu::Bool = true) where {P <: ParameterConfigurations}\n\nReturns B parameteric bootstrap samples of an estimator θ̂ as a p × B matrix, where p is the number of parameters in the statistical model, based on simulated data sets of size m.\n\nThis function requires a method simulate(parameters::P, m::Integer).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.nonparametricbootstrap","page":"Core functions","title":"NeuralEstimators.nonparametricbootstrap","text":"nonparametricbootstrap(θ̂, Z; B::Integer = 100, use_gpu::Bool = true)\nnonparametricbootstrap(θ̂, Z, blocks; B::Integer = 100, use_gpu::Bool = true)\n\nReturns B non-parametric bootstrap samples of an estimator θ̂ as a p × B matrix, where p is the number of parameters in the statistical model.\n\nThe argument blocks caters for block bootstrapping, and should be an integer vector specifying the block for each replicate. For example, if we have 5 replicates with the first two replicates corresponding to block 1 and the remaining replicates corresponding to block 2, then blocks should be [1, 1, 2, 2, 2]. The resampling algorithm tries to produce resampled data sets of a similar size to the original data, but this can only be achieved exactly if the blocks are the same length.\n\n\n\n\n\n","category":"function"}]
}
