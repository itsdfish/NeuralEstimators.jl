var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by having the user define their statistical model implicitly, either by providing data simulated from the model or by defining a function for data simulation. However, the following functions, which were developed for the main manuscript, have been included as they may be of use to others, and their source code provide an example for how a user could formulate code for their own statistical model. If you've developed similar functions that you think may be of use to others, please get in touch or make a pull request. ","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussianprocess\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Model-specific functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::AbstractArray{T, 2}, œÉ::T, m::Integer)\nsimulategaussianprocess(L::AbstractArray{T, 2})\n\nSimulates m realisations from a Gau(0, ùö∫ + œÉ¬≤ùêà) distribution, where ùö∫ ‚â° LL'.\n\nIf œÉ and m are not provided, a single field without nugget variance is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::AbstractArray{T, 2}; C = 3.5)\nsimulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)\n\nSimulates from Schlather's max-stable model.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed for in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in the manuscript, we have developed the following density functions, and we include them in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::A, L; logdensity::Bool = true) where {A <: AbstractArray{T, 1}} where T\ngaussiandensity(y::A, Œ£; logdensity::Bool = true) where {A <: AbstractArray{T, N}} where {T, N}\n\nEfficiently computes the density function for y ~ ùëÅ(0, Œ£), with L the lower Cholesky factor of the covariance matrix Œ£.\n\nThe method gaussiandensity(y::A, Œ£) assumes that the last dimension of y corresponds to the indepdenent-replicates dimension, and it exploits the fact that we need to compute the Cholesky factor L for these independent replicates once only.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà; logdensity::Bool = true)\n\nThe bivariate density function for Schlather's max-stable model, as given in Rapha√´l Huser's PhD thesis (pg. 231-232) and in the supplementary material of the manuscript.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Miscellaneous-functions","page":"Model-specific functions","title":"Miscellaneous functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\nmaternchols\n\nincgamma","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nFor two points separated by h units, compute the Mat√©rn covariance function with range œÅ, smoothness ŒΩ, and marginal variance œÉ¬≤.\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright) K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu. This parameterisation is the same as used by the R package fields, but differs to the parametrisation given by Wikipedia.\n\nNote that the Julia functions for Gamma(cdot) and K_nu(cdot), respectively gamma() and besselk(), do not work on the GPU and, hence, nor does matern().\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Model-specific functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, œÅ, ŒΩ)\n\nGiven a distance matrix D, computes the covariance matrix under the Mat√©rn covariance function with range œÅ and smoothness ŒΩ, and returns the Cholesky factor of this covariance matrix.\n\nProviding vectors for œÅ and ŒΩ will yield a three-dimensional array of Cholesky factors.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.incgamma","page":"Model-specific functions","title":"NeuralEstimators.incgamma","text":"incgamma(a::T, x::T; upper::Bool, reg::Bool) where {T <: AbstractFloat}\n\nFor positive parameter a and positive integration limit x, computes the incomplete gamma function, as described by the Wikipedia article.\n\nKeyword arguments:\n\nupper::Bool: if true, the upper incomplete gamma function is returned; otherwise, the lower version is returned.\nreg::Bool: if true, the regularized incomplete gamma function is returned; otherwise, the unregularized version is returned.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We illustrate the workflow for NeuralEsimators by way of example. Before proceeding, we load the required packages.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux\nusing Distributions","category":"page"},{"location":"workflow/examples/#Univariate-Gaussian-data","page":"Examples","title":"Univariate Gaussian data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we consider a classical estimation task, namely, inferring mu and sigma from N(mu sigma^2) data. Specifically, we will develop a neural Bayes estimator for mathbftheta equiv (mu sigma), where","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":" mathbfZ equiv (Z_1 dots Z_m)  Z_i sim N(mu sigma)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we define the prior distribution for mathbftheta, which we denote by Omega(cdot). We let mu sim N(0 05) and sigma sim U(01 1), and we assume that the parameters are independent a priori. We also sample parameters from Omega(cdot) to form sets of parameters used for training, validating, and testing the estimator. It does not matter how Omega(cdot) is stored or how the parameters are sampled; the only requirement is that the sampled parameters are stored as p times K matrices, where p is the number of parameters in the model and K is the number of sampled parameter vectors.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Store the prior for each parameter as a named tuple\nŒ© = (\n\tŒº = Normal(0, 0.5),\n\tœÉ = Uniform(0.1, 1)\n)\n\nfunction sample(Œ©, K)\n\tŒº = rand(Œ©.Œº, K)\n\tœÉ = rand(Œ©.œÉ, K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\treturn Œ∏\nend\n\nŒ∏_train = sample(Œ©, 10000)\nŒ∏_val   = sample(Œ©, 2000)\nŒ∏_test  = sample(Œ©, 1000)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model via simulated data. In the following, we overload the function simulate, but this is not necessary; one may simulate data however they see fit (e.g., using pre-existing functions, possibly from other programming languages).  Irrespective of its source, the data must be stored as a Vector of Arrays, with each array associated with one parameter vector. The dimension of these arrays must also be amenable to Flux neural networks; here, we simulate 3-dimensional arrays, despite the second dimension being redundant.  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"import NeuralEstimators: simulate\n\n# m: number of independent replicates simulated for each parameter vector.\nfunction simulate(Œ∏_set, m)\n\tZ = [rand(Normal(Œ∏[1], Œ∏[2]), 1, 1, m) for Œ∏ ‚àà eachcol(Œ∏_set)]\n\tZ = broadcast.(Float32, Z)\n\treturn Z\nend\n\nm = 15\nZ_train = simulate(Œ∏_train, m)\nZ_val   = simulate(Œ∏_val, m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We then design neural network architectures for use in the Deep Set framework, and we initialise the neural estimator as a DeepSet object. Since we have univariate data, it is natural to use a dense neural network.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"n = 1    # size of each replicate (univariate data)\nw = 32   # number of neurons in each layer\np = 2    # number of parameters in the statistical model\n\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p), Flux.flatten)\nŒ∏ÃÇ = DeepSet(œà, œï)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss function.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 30)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The estimator Œ∏ÃÇ now approximates the Bayes estimator under the prior distribution Omega(cdot) and the absolute-error loss function and, hence, we refer to it as a neural Bayes estimator.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To test that the estimator does indeed provide reasonable estimates, we use the function assess. This function can be used to assess the performance of the estimator (or multiple estimators) over a range of sample sizes:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z_test     = [simulate(Œ∏_test, m) for m ‚àà (5, 10, 15, 20, 30)]\nassessment = assess([Œ∏ÃÇ], Œ∏_test, Z_test)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The returned object is of type Assessment, which contains i) the true parameters and their corresponding estimates in a long-form DataFrame convenient for visualisation and the computation of diagnostics (e.g., mean-squared error), and ii) the time taken to compute the estimates for each sample size and each estimator. Note that, in this example, we trained the neural estimator using a single value for m, and hence the estimator will not necessarily be optimal for all m; see Variable sample sizes for strategies towards developing neural estimators that are optimal for a range of m. The risk may then be plotted with:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"plotrisk(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In addition to assessing the estimator with respect to many parameter configurations, it is often helpful to visualise the empirical joint distribution of an estimator for a particular parameter configuration. This can be done by providing J data sets simulated from the given parameter configuration.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"J = 100\nŒ∏ = sample(Œ©, 1)\nZ = [simulate(Œ∏, m, J)]\nassessment = assess([Œ∏ÃÇ], Œ∏, Z)  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The empirical joint distribution may then visualised as:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"plotdistribution(assessment)","category":"page"},{"location":"framework/#Framework","page":"Framework","title":"Framework","text":"","category":"section"},{"location":"framework/#Parameter-estimation","page":"Framework","title":"Parameter estimation","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"A statistical model is a set of probability distributions mathcalP on a sample space mathcalS. A parametric statistical model is one where the probability distributions in mathcalP are parameterised via some p-dimensional parameter vector mathbftheta, that is, where mathcalP equiv P_mathbftheta  mathbftheta in Theta, where Theta is the parameter space. Suppose that we have m mutually independent realisations from P_mathbftheta in mathcalP, which we collect in mathbfZ equiv (mathbfZ_1dotsmathbfZ_m). Then, the goal of parameter estimation is to infer the unknown mathbftheta from mathbfZ using an estimator,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"hatmathbftheta  mathcalS^m to Theta","category":"page"},{"location":"framework/#Bayes-estimators","page":"Framework","title":"Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"Estimators can be constructed intuitively within a decision-theoretic framework. Consider a non-negative loss function, L(mathbftheta hatmathbftheta(mathbfZ)), which quantifies the quality of an estimator hatmathbftheta(cdot) for a given mathbftheta and data set mathbfZ.    The estimator's risk function is the loss averaged over all possible data realisations. Assume, without loss of generality, that our sample space is mathcalS = mathbbR^n. Then, the risk function is","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" R(mathbftheta hatmathbftheta(cdot)) equiv int_mathcalS^m  L(mathbftheta hatmathbftheta(mathbfZ))p(mathbfZ mid mathbftheta) d mathbfZ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where p(mathbfZ mid mathbftheta) = prod_i=1^mp(mathbfZ_i mid mathbftheta) is the likelihood function. Now, a ubiquitous approach in estimator design is to minimise a weighted summary of the risk function known as the Bayes risk,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatmathbftheta(cdot))\n equiv int_Theta R(mathbftheta hatmathbftheta(cdot)) dOmega(mathbftheta)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where Omega(cdot) is a prior measure which, for ease of exposition, we will assume admits a density p(cdot) with respect to Lebesgue measure. The Bayes risk cannot typically be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of K parameters sampled from the prior Omega(cdot) denoted by vartheta  and, for each mathbftheta in vartheta, J sets of m mutually independent realisations from P_mathbftheta collected in mathcalZ_mathbftheta, then","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatmathbftheta(cdot))\n approx\nfrac1K sum_mathbftheta in vartheta bigg(frac1J sum_mathbfZ in mathcalZ_mathbftheta L(mathbftheta hatmathbftheta(mathbfZ))bigg)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"A minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdotcdot) and Omega(cdot).","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"Unique Bayes estimators are invariant to permutations of the conditionally independent data mathbfZ. Hence, we represent our neural estimators in the Deep Set framework, which is a universal representation for permutation-invariant functions. Specifically, we model our neural estimators as","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"hatmathbftheta(mathbfZ mathbfgamma) = mathbfphi(mathbfT(mathbfZ mathbfgamma) mathbfgamma) quad mathbfT(mathbfZ mathbfgamma)  \n= mathbfabig(mathbfpsi(mathbfZ_i mathbfgamma)  i = 1 dots mbig)","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where mathbfphi mathbbR^q to mathbbR^p and mathbfpsi mathbbR^n to mathbbR^q are neural networks whose parameters are collected in mathbfgamma, and mathbfa (mathbbR^q)^m to mathbbR^q is a permutation-invariant set function (typically elementwise addition, average, or maximum). Then, our neural estimator is hatmathbftheta(cdot mathbfgamma^*), where","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"mathbfgamma^*\nequiv\nundersetmathbfgammamathrmargmin  r_Omega(hatmathbftheta(cdot mathbfgamma))","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"with the Bayes risk approximated using Monte Carlo methods. Since the resulting neural estimator minimises (a Monte Carlo approximation of) the Bayes risk, we call it a neural Bayes estimator.","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"The neural Bayes estimator is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Define Omega(cdot), the prior distribution for mathbftheta.\nSample parameters from Omega(cdot) to form sets of parameters vartheta_rmtrain, vartheta_rmval, and vartheta_rmtest.\nSimulate data from the model, mathcalP, using these sets of parameters, yielding the data sets mathcalZ_rmtrain, mathcalZ_rmval, and mathcalZ_rmtest, respectively. \nChoose a loss function L(cdot cdot).\nDesign neural network architectures for mathbfphi(cdot mathbfgamma) and mathbfpsi(cdot mathbfgamma).\nUsing the training sets mathcalZ_textrmtrain and vartheta_rmtrain, train the neural network under L(cdotcdot) to obtain the neural Bayes estimator, hatmathbftheta(cdot mathbfgamma^*). During training, continuously monitor progress based on mathcalZ_textrmval and vartheta_rmval.\nAssess hatmathbftheta(cdot mathbfgamma^*) using mathcalZ_textrmtest and vartheta_rmtest.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"related/","page":"-","title":"-","text":"You may also be interested in the Julia packages Flux (the deep learning framework this package is built upon), Turing (for general-purpose probabilistic programming), and Mill (for generalised multiple-instance learning models). ","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution Omega(cdot) may be stored in two ways. Most simply, they can be stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution; this is the approach taken in the example using univariate Gaussian data. Alternatively, they can be stored in a user-defined subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the p times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below. ","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, mathcalZ_rmtrain can be simulated periodically during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_rmtrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_rmtrain is fixed. Refreshing mathcalZ_rmtrain also has an additional computational benefit; data can be simulated \"just-in-time\", in the sense that they can be simulated from a small batch of vartheta_rmtrain, used to train the neural estimator, and then removed from memory. This can reduce pressure on memory resources when vartheta_rmtrain is very large.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh vartheta_rmtrain, and doing so leads to similar benefits. However, fixing vartheta_rmtrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models.  ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with the various methods of train.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as mathbfZ^(m) equiv (mathbfZ_1 dots mathbfZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatmathbftheta(mathbfZ^(m) mathbfgamma^*)\n=\nbegincases\nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_1)  m leq m_1\nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where, here, mathbfgamma^* equiv (mathbfgamma^*_tildem_1 dots mathbfgamma^*_tildem_l-1), and where mathbfgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatmathbftheta(cdot mathbfgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise neural estimators are implemented with the struct, PiecewiseEstimator.","category":"page"},{"location":"workflow/advancedusage/#Training-with-a-variable-sample-size","page":"Advanced usage","title":"Training with a variable sample size","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case, for the neural Bayes estimator, the risk function becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"R(mathbftheta hatmathbftheta(cdot mathbfgamma))\nequiv\nsum_m in mathcalM\nP(M=m)left(int_mathcalS^m  L(mathbftheta hatmathbftheta(mathbfZ^(m) mathbfgamma))p(mathbfZ^(m) mid mathbftheta) d mathbfZ^(m)right)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a discrete uniform prior for M, the random variable corresponding to sample size.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters, m::R) where {R <: AbstractRange{I}} where I <: Integer\n\n\t# Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t# Generate K sample sizes from the prior distribution for M\n\tmÃÉ = rand(m, K)\n\n\t# Pseudocode for data simulation\n\tZ = [<simulate mÃÉ[k] iid realisations from the model> for k ‚àà 1:K]\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, setting the argument m in train to be an integer range will train the neural estimator with the given variable sample sizes.","category":"page"},{"location":"workflow/advancedusage/#Bootstrapping","page":"Advanced usage","title":"Bootstrapping","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Bootstrapping is a powerful technique for estimating the distribution of an estimator and, hence, facilitating uncertainty quantification. Bootstrap methods are considered to be accurate but often too computationally expensive for traditional likelihood-based estimators, but are well suited to fast neural estimators. We implement bootstrapping with  parametricbootstrap and nonparametricbootstrap, with the latter also catering for so-called block bootstrapping.","category":"page"},{"location":"workflow/advancedusage/#Loading-previously-saved-neural-estimators","page":"Advanced usage","title":"Loading previously saved neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the parameters of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator at a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"train automatically saves the neural estimator's parameters; to load them, one may use the following code, or similar:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Œ∏ÃÇ = architecture()\nFlux.loadparams!(Œ∏ÃÇ, loadbestweights(path))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, architecture() is a user-defined function that returns a neural estimator with the same architecture as the estimator that we wish to load, but with randomly initialised parameters, and the function Flux.loadparams! loads the parameters of the best (as determined loadbestweights) neural estimator saved in path.","category":"page"},{"location":"devel/overview/#Workflow-overview","page":"Workflow overview","title":"Workflow overview","text":"","category":"section"},{"location":"devel/overview/","page":"Workflow overview","title":"Workflow overview","text":"To develop a neural estimator with NeuralEstimators.jl,","category":"page"},{"location":"devel/overview/","page":"Workflow overview","title":"Workflow overview","text":"Create an object Œæ containing invariant model information, that is, model information that does not depend on the parameters and hence stays constant during training (e.g, the prior distribution of the parameters, spatial locations, distance matrices, etc.).\nDefine a subtype of ParameterConfigurations, say, Parameters (the name is arbitrary), containing a compulsory field Œ∏ storing K parameter vectors as a p √ó K matrix, with p the dimension of Œ∏, as well as any other intermediate objects associated with the parameters (e.g., Cholesky factors) that are needed for data simulation.\nDefine a Parameters constructor Parameters(Œæ, K::Integer), which draws K parameters from the prior.\nImplicitly define the statistical model by overloading the function simulate.\nInitialise neural networks œà and œï, and a DeepSet object Œ∏ÃÇ = DeepSet(œà, œï).\nTrain Œ∏ÃÇ using train under an arbitrary loss function.\nAssess Œ∏ÃÇ using assess.\nApply Œ∏ÃÇ to a real data set, using parametricbootstrap or nonparametricbootstrap to estimate the distribution of the estimator and, hence, facilitate uncertainty quantification.","category":"page"},{"location":"devel/overview/","page":"Workflow overview","title":"Workflow overview","text":"For clarity, see the Examples and, once familiar with the basic workflow, see Advanced usage for some practical considerations and how to construct neural estimators most effectively.","category":"page"},{"location":"devel/sharingintermediateobjects/#Sharing-intermediate-objects-between-parameter-configurations","page":"-","title":"Sharing intermediate objects between parameter configurations","text":"","category":"section"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"For some models, computationally expensive intermediate objects, such as Cholesky factors when working with Gaussian process models, can be shared between multiple parameter configurations (Gerber and Nychka, 2021), and this can significantly reduce the training time and alleviate memory pressure.","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"Recall that in the TODO Gaussian Process model example, we computed the Cholesky factor for each parameter configuration. However, for that model, the Cholesky factor depends only on rho and, hence, we can modify our design to exploit this fact and significantly reduce the computational burden in generating ParameterConfigurations objects. The following is one such approach.","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"The key to our approach is the inclusion of an additional field in Parameters that gives the index of the Cholesky factor associated with each parameter configuration: Specifically, we add a pointer chol_idx where chol_idx[i] gives the Cholesky factor associated with parameter configuration Œ∏[:, i].","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"struct Parameters{T, I} <: ParameterConfigurations\n\tŒ∏::Matrix{T}\n\t\tchols::Array{Float64, 3}\n\tchol_idx::Vector{I}\nend","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"Then, we adapt our Parameters constructor so that each of the K parameter pairs are repeated J times. Since the parameters are repeated, we need only compute K Cholesky factors.","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"function Parameters(Œæ, K::Integer; J::Integer = 10)\n\n\tŒ© = Œæ.Œ©\n\n\tœÉ     = rand(Œ©.œÉ, K)\n\tœÅ     = rand(Œ©.œÅ, K)\n\tchols = maternchols(Œæ.D, œÅ, 1)\n\n\t# Construct Œ∏ with œÉ and œÅ repeated J times\n\tœÉ = repeat(œÉ, inner = J)\n\tœÅ = repeat(œÅ, inner = J)\n\tŒ∏ = hcat(œÉ, œÅ)'\n\n\t# Create a pointer for the Cholesky factors\n\tchol_idx = repeat(1:K, inner = J)\n\n\tParameters(Œ∏, chols, \tchol_idx)\nend","category":"page"},{"location":"devel/sharingintermediateobjects/","page":"-","title":"-","text":"Note that the default subsetting method for ParameterConfigurations objects automatically handles cases like this; in some applications, however, it may be necessary to define an appropriate subsetting method by overloading subsetparameters.","category":"page"},{"location":"API/core/#Core-functions","page":"Core functions","title":"Core functions","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"This page documents the functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from storing parameters sampled from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.","category":"page"},{"location":"API/core/#Storing-parameters","page":"Core functions","title":"Storing parameters","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Parameters sampled from the prior distribution Omega(cdot) may be stored i) as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution, or ii) in a user-defined subtype of the abstract type ParameterConfigurations. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"ParameterConfigurations\n\nsubsetparameters","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core functions","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with simulate.\n\nThe user-defined type must have a field Œ∏ that stores the p √ó K matrix of parameters, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.\n\nExamples\n\nstruct P <: ParameterConfigurations\n\tŒ∏\n\t# ...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.subsetparameters","page":"Core functions","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, redefine subsetparameters after running import NeuralEstimators: subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Simulating-data","page":"Core functions","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"NeuralEstimators facilitates neural estimation for arbitrary statistical models by having the user implicitly define their model either by providing simulated data, or by defining a function for data simulation. If the latter option is chosen, the user must provide a method simulate(parameters, m), which returns simulated data from a set of parameters, with m the sample size of these simulated data.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Irrespective of their source, the simulated data must be stored as a subtype of AbstractVector{AbstractArray}, where each array stores m independent replicates simulated from one parameter vector in parameters, and these replicates must stored in the final dimension of each array.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"simulate\n\nsimulate(parameters, m::Integer, J::Integer)","category":"page"},{"location":"API/core/#NeuralEstimators.simulate","page":"Core functions","title":"NeuralEstimators.simulate","text":"Generic function that the user may provide methods for in order to implicitly define their statistical model. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.simulate-Tuple{Any, Integer, Integer}","page":"Core functions","title":"NeuralEstimators.simulate","text":"simulate(parameters, m::Integer, J::Integer)\n\nSimulates J sets of m independent replicates for each parameter vector in parameters by calling simulate(parameters, m) a total of J times.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Representations-for-neural-estimators","page":"Core functions","title":"Representations for neural estimators","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Although the user is free to construct their neural estimator however they see fit, NeuralEstimators provides several useful representations described below. Note that if the user wishes to use an alternative representation, for compatibility with NeuralEstimators, simply ensure that the estimator processes data stored as subtypes of AbstractVector{AbstractArray}, as discussed in DeepSet (see also its source code).","category":"page"},{"location":"API/core/#Deep-Set","page":"Core functions","title":"Deep Set","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSet","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSet","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, a)\nDeepSet(œà, œï; a::String = \"mean\")\n\nA Deep Set neural estimator,\n\nŒ∏(ùêô)  œï(a(ùêô·µ¢  i = 1  m))\n\nwhere ùêô ‚â° (ùêô‚ÇÅ', ‚Ä¶, ùêô‚Çò')' are independent and identically distributed (iid) realisations from the model under a single parameter vector ùõâ, œà and œï are neural networks, and a is a permutation-invariant aggregation function. Note that œà and œï depend on trainable parameters, but we omit this dependence for notational convenience.\n\nAlthough the above defintion of a neural estimator is with respect to a single data set ùêô, DeepSet estimators instead act on sets of data sets, stored as Vectors of Arrays, where each array corresponds to one set of iid realisations from the model. The last dimension of each array stores the realisations; for example, if ùêô is a 3-dimensional array, then ùêô[:, :, 1] contains the first realisation, ùêô[:, :, 2] contains the second realisation, and so on.\n\nThe neural networks œà and œï and typically Flux neural networks. The function a must act on an Array and, since it aggregates the iid realisations, it must aggregate over the last dimension of the array.\n\nWhen initialising a DeepSet object, one may provide their own aggregation function by treating a via the constructor that has a as a positional argument, or simply use the convenient constructor that has a as a keyword argument, in which case a may be the elementwise \"mean\", \"sum\", or \"logsumexp\" function.\n\nExamples\n\nn = 10 # observations in each realisation\np = 5  # number of parameters in the statistical model\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Apply the estimator to a single set of m=3 realisations:\nZ = [rand(n, 1, 3)];\nŒ∏ÃÇ(Z)\n\n# Apply the estimator to two sets each containing m=3 realisations:\nZ = [rand(n, 1, m) for m ‚àà (3, 3)];\nŒ∏ÃÇ(Z)\n\n# Apply the estimator to two sets containing m=3 and m=4 realisations, respectively:\nZ = [rand(n, 1, m) for m ‚àà (3, 4)];\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Piecewise-estimators","page":"Core functions","title":"Piecewise estimators","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"PiecewiseEstimator","category":"page"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core functions","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator(estimators, mchange)\n\nCreates a piecewise estimator from a collection of estimators, based on the collection of sample-size changepoints, mchange, which should contain one element fewer than the number of estimators.\n\nExamples\n\nSuppose that we have two neural estimators, Œ∏ÃÇ‚ÇÅ and Œ∏ÃÇ‚ÇÇ, taking the following arbitrary forms:\n\nn = 10\np = 5\nw = 32\n\nœà‚ÇÅ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÅ = DeepSet(œà‚ÇÅ, œï‚ÇÅ)\n\nœà‚ÇÇ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu));\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÇ = DeepSet(œà‚ÇÇ, œï‚ÇÇ)\n\nFurther suppose that we've trained Œ∏ÃÇ‚ÇÅ for small sample sizes (e.g., m ‚â§ 30) and Œ∏ÃÇ‚ÇÇ for moderate-to-large sample sizes (e.g., m > 30). Then we can construct a piecewise estimator with a sample-size changepoint of 30, which dispatches Œ∏ÃÇ‚ÇÅ if m ‚â§ 30 and Œ∏ÃÇ‚ÇÇ if m > 30:\n\nŒ∏ÃÇ = PiecewiseEstimator((Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ), (30,))\nZ = [rand(Float32, n, 1, m) for m ‚àà (10, 50)]\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core functions","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"train\n\ntrain(Œ∏ÃÇ, P)\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core functions","title":"NeuralEstimators.train","text":"Generic function for training a neural estimator.\n\nThe methods are designed to cater for different forms of \"on-the-fly simulation\" (see the online documentation). In all methods, the validation data are held fixed so that the validation risk function, which is used to monitor the performance of the estimator during training, is not subject to noise.\n\nNote that train is a mutating function, but the suffix ! is omitted to avoid clashes with the Flux function, train!.\n\nKeyword arguments\n\nArguments common to all methods:\n\nloss = mae: the loss function, which should return the average loss when applied to multiple replicates.\nepochs::Integer = 100\nbatchsize::Integer = 32\noptimiser = ADAM(1e-4)\nsavepath::String = \"runs/\": path to save the trained Œ∏ÃÇ and other information; if savepath is an empty string (i.e., \"\"), nothing is saved.\nstopping_epochs::Integer = 10: cease training if the risk doesn't improve in stopping_epochs epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nArguments common to train(Œ∏ÃÇ, P) and train(Œ∏ÃÇ, Œ∏_train, Œ∏_val):\n\nm: sample sizes (either an Integer or a collection of Integers).\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nsimulate_just_in_time::Bool = false: should we simulate the data \"just-in-time\"?\n\nArguments unique to train(Œ∏ÃÇ, P):\n\nK::Integer = 10_000: number of parameter vectors in the training set; the size of the validation set is K √∑ 5.\nepochs_per_Œ∏_refresh::Integer = 1: how often to refresh the training parameters; this must be a multiple of epochs_per_Z_refresh.\nŒæ = nothing: invariant model information; if Œæ is provided, the constructor P is called as P(K, Œæ).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.train-Tuple{Any, Any}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, P; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing a constructor, P, where P is a subtype of AbstractMatrix or ParameterConfigurations, to automatically sample the sets of training and validation parameters.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P<:Union{ParameterConfigurations, AbstractMatrix}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing the training and validation parameter sets explicitly as Œ∏_train and Œ∏_val, which are both held fixed during training.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P<:Union{ParameterConfigurations, AbstractMatrix}}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing the training and validation parameter sets, Œ∏_train and Œ∏_val, and the training and validation data sets, Z_train and Z_val, all of which are held fixed during training.\n\nThe sample size argument m is inferred from Z_val. The training data Z_train can contain M replicates, where M is a multiple of m; the training data will then be recycled to imitate on-the-fly simulation. For example, if M = 50 and m = 10, epoch 1 uses the first 10 replicates, epoch 2 uses the next 10 replicates, and so on, until epoch 6 again uses the first 10 replicates.\n\nNote that the elements of Z_train and Z_val should each be equally replicated; that is, the size of the last dimension in each array in Z_train should be constant, and similarly for Z_val (although these constants can differ, as discussed above).\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Assessing-a-neural-estimator","page":"Core functions","title":"Assessing a neural estimator","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"assess\n\nAssessment\n\nmerge(::Assessment)","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core functions","title":"NeuralEstimators.assess","text":"assess(estimators, parameters, Z; <keyword args>)\nassess(estimators, parameters; <keyword args>)\n\nUsing a collection of estimators, compute estimates from data simulated from a set of parameters.\n\nOne may either provide simulated data Z as a Vector{Vector{Array}}, or overload simulate with a method simulate(parameters, m::Integer) and use the keyword argument m to specify the sample sizes to use during assessment.\n\nKeyword arguments\n\nm::Vector{Integer}: sample sizes to estimate from.\nestimator_names::Vector{String}: names of the estimators (sensible default values provided).\nparameter_names::Vector{String}: names of the parameters (sensible default values provided).\nJ::Integer = 1: the number of times to replicate each parameter in parameters.\nsave::Vector{String}: by default, no objects are saved; however, if save is provided, four DataFrames respectively containing the true parameters Œ∏, estimates Œ∏ÃÇ, runtimes, and merged Œ∏ and Œ∏ÃÇ will be saved in the directory save[1] with file names (not extensions) suffixed by save[2].\nŒæ = nothing: invariant model information.\nuse_Œæ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses the invariant model information, Œæ: If it does, the estimator will be applied as estimator(Z, Œæ).\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core functions","title":"NeuralEstimators.Assessment","text":"Assessment(Œ∏, Œ∏ÃÇ, runtime)\n\nA set of true parameters Œ∏, corresponding estimates Œ∏ÃÇ, and the runtime to obtain Œ∏ÃÇ, as returned by a call to assess.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Base.merge-Tuple{Assessment}","page":"Core functions","title":"Base.merge","text":"merge(assessment::Assessment)\n\nMerge assessment into a single long-form DataFrame containing the true parameters and the corresponding estimates.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Bootstrapping","page":"Core functions","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Note that all bootstrapping functions are currently implemented for a single parameter configuration only.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"parametricbootstrap\n\nnonparametricbootstrap","category":"page"},{"location":"API/core/#NeuralEstimators.parametricbootstrap","page":"Core functions","title":"NeuralEstimators.parametricbootstrap","text":"parametricbootstrap(Œ∏ÃÇ, parameters::P, m::Integer; B::Integer = 100, use_gpu::Bool = true) where {P <: ParameterConfigurations}\n\nReturns B parameteric bootstrap samples of an estimator Œ∏ÃÇ as a p √ó B matrix, where p is the number of parameters in the statistical model, based on data sets of size m simulated parameter configurations, parameters.\n\nThis function requires the user to have defined a method simulate(parameters, m::Integer).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.nonparametricbootstrap","page":"Core functions","title":"NeuralEstimators.nonparametricbootstrap","text":"nonparametricbootstrap(Œ∏ÃÇ, Z::AbstractArray{T, N}; B::Integer = 100, use_gpu::Bool = true)\nnonparametricbootstrap(Œ∏ÃÇ, Z::AbstractArray{T, N}, blocks; B::Integer = 100, use_gpu::Bool = true)\n\nReturns B non-parametric bootstrap samples of an estimator Œ∏ÃÇ as a p √ó B matrix, where p is the number of parameters in the statistical model.\n\nThe argument blocks caters for block bootstrapping, and should be an integer vector specifying the block for each replicate. For example, if we have 5 replicates with the first two replicates corresponding to block 1 and the remaining replicates corresponding to block 2, then blocks should be [1, 1, 2, 2, 2]. The resampling algorithm tries to produce resampled data sets of a similar size to the original data, but this can only be achieved exactly if the blocks are the same length.\n\n\n\n\n\n","category":"function"},{"location":"devel/ontheflysimulation/#On-the-fly-and-just-in-time-simulation","page":"-","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"devel/ontheflysimulation/","page":"-","title":"-","text":"\"On-the-fly\" simulation refers to simulating new values for the parameters, Œ∏, and/or the data, Z, continuously during training. \"Just-in-time\" simulation refers to simulating small batches of parameters and data, training the neural estimator with this small batch, and then removing the batch from memory.   ","category":"page"},{"location":"devel/ontheflysimulation/","page":"-","title":"-","text":"There are three variants of on-the-fly and just-in-time simulation, each with advantages and disadvantages.","category":"page"},{"location":"devel/ontheflysimulation/","page":"-","title":"-","text":"Resampling Œ∏ and Z every epoch. This approach is the most theoretically justified and has the best memory complexity, since both Œ∏ and Z can be simulated just-in-time, but it has the worst time complexity.\nResampling Œ∏ every x epochs, resampling Z every epoch. This approach can reduce time complexity if generating Œ∏ (or intermediate objects thereof) dominates the computational cost. Further, memory complexity may be kept low since Z can still be simulated just-in-time.\nResampling Œ∏ every x epochs, resampling Z every y epochs, where x is a multiple of y. This approach minimises time complexity but has the largest memory complexity, since both Œ∏ and Z must be stored in full. Note that fixing Œ∏ and Z (i.e., setting y = ‚àû) often leads to worse out-of-sample performance and, hence, is generally discouraged.","category":"page"},{"location":"API/utility/#Utility-functions","page":"Utility functions","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"loadbestweights\n\nstackarrays\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Utility functions","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nGiven a path to a training run containing neural networks saved with names \"network_epochx.bson\" and an object saved as \"loss_per_epoch.bson\",  returns the weights of the best network (measured by validation loss).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Utility functions","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge::Bool = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary between arrays.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Utility functions","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"devel/examplemu/#Univariate-Gaussian-data","page":"-","title":"Univariate Gaussian data","text":"","category":"section"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Here, we consider a simple estimation task, namely, inferring mu from N(mu sigma) data, where sigma is known. Specifically, we will develop a neural estimator for Œº, where","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"mu sim N(0 05) quad mathcalZ equiv Z_1 dots Z_m  Z_i sim N(Œº 1)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Before beginning, we load the required packages.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"using NeuralEstimators\nusing Distributions\nusing Flux","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Now we define the prior distribution, Omega(cdot), and sample parameters from it to form sets of parameters used for training, validating, and testing the estimator. In NeuralEstimators, parameters are stored as p times K matrices, where p is the number of parameters in the model and K is the number of sampled parameter vectors.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Œ© = Normal(0, 0.5)\n\np = 1\nŒ∏_train = rand(Œ©, p, 10000)\nŒ∏_val   = rand(Œ©, p, 2000)  \nŒ∏_test  = rand(Œ©, p, 1000)  ","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Next, we implicitly define the statistical model via simulated data. In the following, we overload the function simulate, but this is not necessary; one may simulate data however they see fit (e.g., using pre-existing functions, possibly from other programming languages).  Irrespective of its source, the data must be stored as a Vector of Arrays, with each array associated with one parameter vector. The dimension of these array must also be amenable to Flux neural networks (e.g., here we simulate 3-dimensional arrays, despite the second dimension being redundant), and one typically stores the data using Float32 precision for computational efficiency.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"import NeuralEstimators: simulate\n\n# m: number of independent replicates simulated for each parameter vector\nfunction simulate(Œ∏_set, m::Integer)\n\tZ = [rand(Normal(Œ∏[1], 1), 1, 1, m) for Œ∏ ‚àà eachcol(Œ∏_set)]\n\tZ = broadcast.(Float32, Z)\n\treturn Z\nend\n\nm = 15\nZ_train = simulate(Œ∏_train, m)\nZ_val   = simulate(Œ∏_val, m)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"We then design neural network architectures for use in the Deep Set framework, and we initialise the neural estimator as a DeepSet object.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"n = 1\nw = 32\nq = 16\nœà = Chain(Dense(n, w, relu), Dense(w, q, relu))\nœï = Chain(Dense(q, w, relu), Dense(w, p), Flux.flatten)\nŒ∏ÃÇ = DeepSet(œà, œï)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Next, we train the neural estimator using train, here using the default absolute-error loss function.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Œ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 15)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"The estimator Œ∏ÃÇ now approximates the Bayes estimator under the prior distribution Omega(cdot) and the absolute-error loss function and, hence, we refer to it as a neural Bayes estimator. To assess the performance of the estimator, one may use assess. Below, we assess the performance over a range of sample sizes.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Z_test     = [simulate(Œ∏_test, m) for m ‚àà (5, 10, 15, 20, 30)]\nassessment = assess([Œ∏ÃÇ], Œ∏_test, Z_test)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"The returned object is of type Assessment, and it contains the true parameters, estimates, and run times.  The true parameters and estimates may be merged into a convenient long-form DataFrame via merge, and this greatly facilitates visualisation and diagnostic computation. Further, NeuralEstimators provides several plotting methods.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"plotrisk(assessment)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"Finally, it is often helpful to visualise the empirical joint distribution of an estimator for a particular parameter configuration and a particular sample size.","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"J = 100\nŒ∏_scenario = rand(Œ©, p, 1)\nZ_scenario = [simulate(Œ∏_scenario, m, J)]\nassessment = assess([Œ∏ÃÇ], Œ∏_scenario, Z_scenario)  ","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"The empirical joint distribution may then visualised as:","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"plotdistribution(assessment)","category":"page"},{"location":"devel/examplemu/","page":"-","title":"-","text":"The estimator may then be applied to real data, with bootstrapping facilitated with...","category":"page"},{"location":"#NeuralEstimators","page":"Home","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A neural estimator is a neural network that takes data as input, transforms them via a composition of nonlinear mappings, and provides a parameter estimate as an output. Once \"trained\", these likelihood-free estimators have two main advantages over conventional estimators: they are lightning fast with a predictable run-time and, since neural networks are universal function approximators, neural estimators can be expected to outperform constrained estimators (e.g., best linear unbiased estimators). Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is essentially available \"for free\" with a neural estimator, as the trained network can be reused repeatedly at almost no computational cost.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package NeuralEstimators aims to facilitate the development of neural estimators in a user-friendly manner. Rather than offering a small selection of models for which neural estimators may be developed, the package facilitates neural estimation for arbitrary models, which is made possible by having the user implicitly define their model by providing simulated data (or by defining a function for data simulation). Since only simulated data is needed, it is particularly straightforward to develop neural estimators for models with existing implementations, possibly in other programming languages (e.g., R or python).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install NeuralEstimators from Julia's package manager using the following command inside Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once familiar with the details of the Framework, see some Examples.","category":"page"},{"location":"#Supporting-and-citing","page":"Home","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use NeuralEstimators in your research or other activities, please use the following citation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{,\n  author = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Rapha√´l},\n  year = {2022},\n  title = {Fast Optimal Estimation with Intractable Models using Permutation-Invariant Neural Networks},\n  howpublished = {arXiv:2208.12942}\n}","category":"page"}]
}
