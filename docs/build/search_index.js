var documenterSearchIndex = {"docs":
[{"location":"#Example.jl-Documentation","page":"Example.jl Documentation","title":"Example.jl Documentation","text":"","category":"section"},{"location":"","page":"Example.jl Documentation","title":"Example.jl Documentation","text":"","category":"page"},{"location":"#Functions","page":"Example.jl Documentation","title":"Functions","text":"","category":"section"},{"location":"","page":"Example.jl Documentation","title":"Example.jl Documentation","text":"func(x)","category":"page"},{"location":"#NeuralEstimators.func-Tuple{Any}","page":"Example.jl Documentation","title":"NeuralEstimators.func","text":"General work flow\n\nThe first step is to define Ω, the prior distribution for the p-dimensional parameter vector, θ. This can be an object of any type. We also need a method sample(Ω, K) that returns K samples of θ as a p × K matrix.\n\nΩ = Normal(0, 0.5)\nsample(Ω, K) = rand(Ω, K)'\n\nNext, we define a struct, whose name is arbitrary, which must have the field θ but can store other information. Storing information in a struct is somewhat redundant for the current simple model but is useful for more complicated models, as will be shown later, and it is needed for the different variants of on-the-fly and just-in-time simulation, also discussed later. So, for consistency, we require it across the board. We also define a constructor.\n\nstruct Parameters <: ParameterConfigurations\n    θ\nend\n\nNext, we implicitly define the statistical model by providing a simulate() method for data simulation conditional on θ. The function takes two arguments, θ and m, with the type of m depending on the values the estimator will be used for (e.g., a single sample size, in which case m will be an Integer). Irrespective of the type of m, simulate() should return a a vector of (multi-dimensional) arrays.\n\nsimulate(params::Parameters, m::Integer) = [rand(Normal(t, 1), θ) for t ∈ params.θ]\n\nWe then choose an architecture for modelling ψ(⋅) and ϕ(⋅) in the Deep Set framework, and initialise the neural estimator as a DeepSet object.\n\np = 1\nw = 32\nq = 16\nψ = Chain(Dense(n, w, relu), Dense(w, q, relu))\nϕ = Chain(Dense(q, w, relu), Dense(w, p), flatten)\nθ̂ = DeepSet(ψ, ϕ)\n\nTrain the neural estimator using train(). This optimisation is performed with respect to an arbitrary loss function L (default absolute error loss).\n\nθ̂ = train(θ̂, Ω, m = 1, ...)\n\nθ̂ now approximates the Bayes estimator for θ. The performance of θ̂ can be tested using the function estimate(), which tests θ̂ on a set of testing parameters sampled from Ω. The estimates are saved as a .csv file for convenience.\n\nestimate(θ̂, Ω, ...)\n\nVariants of on-the-fly and just-in-time simulation\n\nThe above approach assumes that θ and Z are continuously refreshed every epoch. This approach is the simplest, most theoretically justified, and has the best memory complexity, since both θ and Z can be simulated just-in-time; however, it is also the most time expensive. There are two alternatives to this approach:\n\nRefresh θ every x epochs, refresh Z every epoch. This can reduce time complexity if generating parameters involves computationally expensive terms, such as Cholesky factors, and memory complexity may be kept low since Z can still be simulated just-in-time.\nrefresh θ every x epochs, refresh Z every y>x epochs. This minimises time complexity but has the large memory complexity, since both θ and Z cannot be simulated just-in-time and must be stored in full.\n\nTo cater for these variants, another train() method is available, which takes the additional argument params (discussed below), and the keyword arguments epochs_per_θ_refresh and epochs_per_Z_refresh.\n\nThe argument params should be a struct whose only requirement is a field named θ, which stores the parameters. In addition to being needed for the simulation variants discussed above, this struct is also useful in more complicated models when re-using expensive terms, like Cholesky factors, can significantly reduce the computational burden. The name of the struct is arbitrary, but it is usually convenient to name it in reflection of the statistic model.\n\nstruct NormalParameterConfigurations\n    θ\nend\n\nThen, the neural estimator is trained as before.\n\nθ̂ = train(θ̂, Ω, NormalParameterConfigurations, epochs_per_θ_refresh = 5, epochs_per_Z_refresh = 10)\n\n\n\n\n\n","category":"method"},{"location":"","page":"Example.jl Documentation","title":"Example.jl Documentation","text":"link to Example.jl Documentation\nlink to func(x)","category":"page"},{"location":"#Index","page":"Example.jl Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"Example.jl Documentation","title":"Example.jl Documentation","text":"","category":"page"}]
}
